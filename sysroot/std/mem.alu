//! Working with memory (allocation, slices, copying, ...)
//!
//! The main feature of this module is the [slice] type, which is a pointer to a region
//! of memory. `&[T]` is a slice (read-only) of `T`, and `&mut [T]` is a mutable slice.

/// Types that can be "freed".
///
/// Usually these types contain something that is heap-allocated.
protocol Freeable<Self> {
    /// Frees the memory backing the object.
    fn free(self: &mut Self);
}

/// Types that implement move semantics
///
/// If a type allocates memory, it is a good idea to implement move function, so that instances
/// can be passed around by value, leaving inert instances behind. This is chiefly used to `defer`
/// expressions are more ergonomic.
///
/// ```
/// use std::collections::Vector;
///
/// fn do_something(v: Vector<i32>) {
///     // ...
///     v.free();
/// }
///
/// let foo: Vector<i32> = Vector::new();
/// defer foo.free(); // Clean up foo if function returns early
/// // ...
/// do_something(foo.move());
/// // will not lead to double free when this function returns.
/// ```
protocol Movable<Self> {
    /// Creates a copy of the object, leaving an inert object behind.
    ///
    /// See [Movable] for details.
    fn move(self: &mut Self) -> Self;
}

/// Types that be copied non-trivially (usually types containing a heap allocation).
protocol Clonable<Self> {
    /// Returns a copy of the object.
    fn clone(self: &Self) -> Self;
}

/// Types that can be viewed as const slices
///
/// Types that implement this trait can be indexed as if they were slices.
///
/// ## Example
/// ```
/// use std::collections::Vector;
///
/// let v: Vector<i32> = Vector::new();
/// v.push(1);
/// v.push(2);
/// v.push(3);
///
/// assert_eq!(v[1], 2);
/// ```
protocol AsSlice<Self, T> {
    /// Returns the contents of the object as a slice.
    ///
    /// See [AsSlice] for details.
    fn as_slice(self: &Self) -> &[T];
}

/// Types that can be viewed as mutable slices
///
/// Types that implement this trait can be indexed as if they were slices.
///
/// ## Example
/// ```
/// use std::collections::Vector;
///
/// let v: Vector<i32> = Vector::new();
/// v.push(1);
/// v.push(2);
/// v.push(3);
///
/// v[2] = 4;
///
/// assert_eq!(v.pop(), Option::some(4));
/// ```
protocol AsSliceMut<Self: AsSlice<Self, T>, T> {
    /// Returns the contents of the object as a mutable slice.
    ///
    /// See [AsSliceMut] for details.
    fn as_slice_mut(self: &mut Self) -> &mut [T];
}

/// Fat pointers to a contiguous region of memory.
///
/// ```pseudo_alumina
/// type &[T] = slice<&T>;
/// type &mut [T] = slice<&mut T>;
/// ```
///
/// Slice fat "pointers" are just regular structs that compiler handles in a special way with regards
/// to syntax, implicit coercion and type inference. They are generic over the pointer-to-element type
/// rather than the element type itself. This is an implementation detail to ensure that `&mut [T]` and
/// `&[T]` are distinguished without having to have two distinct types for mutable and const slices.
#[lang(slice)]
struct slice<Ptr: builtins::Pointer> {
    _ptr: Ptr,
    _len: usize,
}

impl slice {
    use builtins::Pointer;
    use cmp::{Equatable, Comparable, Ordering};
    use hash::{Hasher, Hashable};
    use builtins::{Primitive, PointerOf, ZeroSized};
    use libc::memcmp;

    /// Empty slice
    #[inline(always)]
    fn empty<Ptr: Pointer>() -> slice<Ptr> {
        slice::<Ptr> { _ptr: dangling::<Ptr>(), _len: 0 }
    }

    /// Create a slice from a pointer and length
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let arr = [1, 2, 3];
    /// let ptr: &i32 = &arr[0];
    ///
    /// let slice = slice::from_raw(ptr, 2);
    /// assert_eq!(slice, &[1, 2]);
    /// ```
    #[inline(ir)]
    #[lang(slice_new)]
    fn from_raw<Ptr: Pointer>(ptr: Ptr, len: usize) -> slice<Ptr> {
        slice::<Ptr> { _ptr: ptr, _len: len }
    }

    /// Returns the length of the slice
    #[inline(ir)]
    fn len<Ptr: Pointer>(self: slice<Ptr>) -> usize {
        self._len
    }

    /// Returns a pointer to the first element of the slice.
    ///
    /// Unlike `&slice[0]`, this is not range-checked, so it will not panic
    /// if the slice is empty, but may return `null`, a dangling or an otherwise
    /// invalid pointer.
    #[inline(always)]
    fn as_ptr<Ptr: Pointer>(self: slice<Ptr>) -> Ptr {
        self._ptr
    }

    /// Returns `true` if the slice has a length of 0, and `false` otherwise.
    #[inline]
    fn is_empty<Ptr: Pointer>(self: slice<Ptr>) -> bool {
        self._len == 0
    }

    /// Allocates an array of specified size on the heap.
    ///
    /// The values are not initialized.
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let hw = "Hello, World!";
    /// let s = slice::alloc::<u8>(hw.len());
    /// defer s.free();
    ///
    /// hw.copy_to(&s[0]);
    /// assert_eq!(hw, s);
    /// ```
    fn alloc<T>(len: usize) -> &mut [T] {
        #[allow(constant_condition)]
        let ptr = when typing::is_zero_sized::<T>() {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            std::intrinsics::const_alloc::<T>(len)
        } else {
            libc::malloc(size_of::<T>() * len) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Resizes a heap-allocated array.
    fn realloc<T>(slice: &mut [T], len: usize) -> &mut [T] {
        #[allow(constant_condition)]
        let ptr = when slice[0] is builtins::ZeroSized {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            let new = std::intrinsics::const_alloc::<T>(len);
            slice.copy_to_nonoverlapping(new);
            slice.free();
            new
        } else {
            libc::realloc(slice._ptr as &mut void, size_of::<T>() * len) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Allocates an array of specified size on the heap.
    ///
    /// The values are zero-initialized.
    fn alloc_zeroed<T>(len: usize) -> &mut [T] {
        let ptr = when typing::matches::<T, builtins::ZeroSized>() {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            std::intrinsics::const_alloc_zeroed::<T>(len)
        } else {
            libc::calloc(len, size_of::<T>()) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Copies a region of memory from `src` to `dst`.
    ///
    /// The memory ranges must not overlap, use [copy_to] if they may be overlapping.
    fn copy_to_nonoverlapping<T>(src: &[T], dst: &mut T) {
        when !(src[0] is builtins::ZeroSized) {
            #[allow(constant_condition)]
            if runtime::in_const_context() {
                for i in 0usize..src.len() {
                    *(dst + i) = src[i];
                }
            } else {
                libc::memcpy(dst as &mut void, src._ptr as &void, src.len() * size_of::<T>());
            }
        }
    }

    /// Copies a region of memory from `src` to `dst`.
    ///
    /// The ranges may overlap.
    fn copy_to<T>(src: &[T], dst: &mut T) {
        when !(src[0] is builtins::ZeroSized) {
            #[allow(constant_condition)]
            if runtime::in_const_context() {
                for i in 0usize..src.len() {
                    *(dst + i) = src[i];
                }
            } else {
                libc::memmove(dst as &mut void, src._ptr as &void, src.len() * size_of::<T>());
            }
        }
    }

    /// Returns the element at the given index.
    ///
    /// If the index is out of bounds, the function will return `None`.
    fn get<T>(self: &[T], index: usize) -> Option<T> {
        if index < self.len() {
            Option::some(*(self._ptr + index))
        } else {
            Option::none()
        }
    }

    /// Copy a slice into a fixed-size array.
    ///
    /// The length of the result array must exactly match the length of the slice.
    ///
    /// ## Example
    /// ```
    /// let hw = "Hello, world";
    /// let hw: [u8; 5] = hw[..5].to_array();
    ///
    /// assert_eq!(hw[..] as &[u8], "Hello");
    /// ```
    fn to_array<T, Arr: builtins::ArrayOf<T>>(self: &[T]) -> Arr {
        let ret: Arr;
        debug_assert!(self.len() == ret.len());
        self.copy_to_nonoverlapping(&ret[0]);
        ret
    }

    /// @ std::iter::Iterable::iter
    #[inline(always)]
    fn iter<Ptr: Pointer>(self: slice<Ptr>) -> SliceIterator<Ptr> {
        SliceIterator { inner: self }
    }

    /// @ std::iter::IterableRef::iter_ref
    #[inline(always)]
    fn iter_ref<Ptr: Pointer>(self: slice<Ptr>) -> SliceRefIterator<&builtins::deref_of<Ptr>> {
        SliceRefIterator { inner: self as slice<&builtins::deref_of<Ptr>> }
    }

    /// @ std::iter::IterableMut::iter_mut
    #[inline(always)]
    fn iter_mut<T>(self: &mut [T]) -> SliceRefIterator<&mut T> {
        SliceRefIterator { inner: self }
    }

    /// Fill slice with a value
    fn fill<T>(slice: &mut [T], value: T) {
        when slice[0] is ZeroSized {
            // nop
        } else when slice[0] is u8 {
            libc::memset(slice._ptr as &mut void, value as libc::c_int, slice.len());
        } else {
            for i in 0..slice.len() {
                slice[i] = value;
            }
        }
    }

    /// @ std::cmp::Equatable::equals
    fn equals<T: Equatable<T>, Ptr: PointerOf<T>>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> bool {
        if lhs.len() != rhs.len() {
            return false;
        }

        when *lhs._ptr is ZeroSized {
            true
        } else when *lhs._ptr is Primitive {
            // Optimization for slices of primitive types
            memcmp(lhs._ptr as &void, rhs._ptr as &void, lhs.len() * size_of::<T>()) == 0
        } else {
            let idx = 0usize;
            while idx < lhs.len() {
                if (*lhs)[idx] != (*rhs)[idx] {
                    return false;
                }
                idx += 1;
            }
            true
        }
    }

    /// @ std::cmp::Comparable::compare
    fn compare<T: Comparable<T>, Ptr: PointerOf<T>>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> Ordering {
        use cmp::min;

        when *lhs._ptr is ZeroSized {
            lhs.len().compare(&rhs.len())
        } else when *lhs._ptr is u8 {
            let cmp = memcmp(lhs._ptr as &void, rhs._ptr as &void, min(lhs.len(), rhs.len()) * size_of::<T>());
            if cmp < 0 {
                Ordering::Less
            } else if cmp > 0 {
                Ordering::Greater
            } else {
                lhs.len().compare(&rhs.len())
            }
        } else {
            let idx = 0usize;
            let len = min(lhs.len(), rhs.len());
            while idx < len {
                let cmp = (*lhs)[idx].compare(&(*rhs)[idx]);
                if cmp != Ordering::Equal {
                    return cmp;
                }
                idx += 1;
            }
            lhs.len().compare(&rhs.len())
        }
    }

    /// @ std::hash::Hashable::hash
    fn hash<T: Hashable<T, H>, Ptr: PointerOf<T>, H: Hasher<H>>(self: &slice<Ptr>, hasher: &mut H) {
        when *self._ptr is ZeroSized {
            // no-op
        } else when *self._ptr is u8 {
            hasher.write(*self);
        } else {
            let idx = 0usize;
            while idx < self.len() {
                (*self)[idx].hash(hasher);
                idx += 1;
            }
        }
    }

    /// @ std::fmt::Formattable::fmt
    fn fmt<Ptr: PointerOf<u8>, F: fmt::Formatter<F>>(self: &slice<Ptr>, f: &mut F) -> Result<(), fmt::Error> {
        f.write_str(*self)
    }

    /// Frees the slice.
    ///
    /// This requires that the slice was allocated with `alloc` or `alloc_zeroed`.
    fn free<T>(a: &mut [T]) {
        when !(*a._ptr is builtins::ZeroSized) {
            #[allow(constant_condition)]
            if runtime::in_const_context() {
                std::intrinsics::const_free::<T>(a._ptr);
            } else {
                libc::free(a._ptr as &mut void);
            }
        }
    }

    mixin<T: Equatable<T>, Ptr: PointerOf<T>> Equatable<slice<Ptr>>;
    mixin<T: Comparable<T>, Ptr: PointerOf<T>> Comparable<slice<Ptr>>;
}

/// Iterator over elements of a slice
struct SliceIterator<Ptr: builtins::Pointer> {
    inner: slice<Ptr>,
}

impl SliceIterator<Ptr: builtins::Pointer> {
    /// @ iter::Iterator::next
    #[inline(always)]
    fn next(self: &mut SliceIterator<Ptr>) -> Option<builtins::deref_of<Ptr>> {
        if self.inner.len() > 0 {
            let result = Option::some(*self.inner._ptr);
            self.inner._ptr = self.inner._ptr + 1;
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::DoubleEndedIterator::next_back
    #[inline(always)]
    fn next_back(self: &mut SliceIterator<Ptr>) -> Option<builtins::deref_of<Ptr>> {
        if self.inner.len() > 0 {
            let result = Option::some(self.inner[self.inner.len() - 1]);
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::Iterator::size_hint
    fn size_hint(self: &SliceIterator<Ptr>) -> Option<usize> {
        Option::some(self.inner.len())
    }

    mixin iter::Iterator<SliceIterator<Ptr>, builtins::deref_of<Ptr>>;
    mixin iter::IteratorExt<SliceIterator<Ptr>, builtins::deref_of<Ptr>>;
    mixin iter::DoubleEndedIterator<SliceIterator<Ptr>, builtins::deref_of<Ptr>>;
    mixin iter::DoubleEndedIteratorExt<SliceIterator<Ptr>, builtins::deref_of<Ptr>>;
}

/// Iterator over pointers to elements of a slice
struct SliceRefIterator<Ptr: builtins::Pointer> {
    inner: slice<Ptr>,
}

impl SliceRefIterator<Ptr: builtins::Pointer> {
    /// @ iter::Iterator::next
    #[inline(always)]
    fn next(self: &mut SliceRefIterator<Ptr>) -> Option<Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(self.inner._ptr);
            self.inner._ptr = self.inner._ptr + 1;
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::DoubleEndedIterator::next_back
    #[inline(always)]
    fn next_back(self: &mut SliceRefIterator<Ptr>) -> Option<Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(&self.inner[self.inner.len() - 1]);
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::Iterator::size_hint
    fn size_hint(self: &SliceRefIterator<Ptr>) -> Option<usize> {
        Option::some(self.inner.len())
    }

    mixin iter::Iterator<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::IteratorExt<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::DoubleEndedIterator<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::DoubleEndedIteratorExt<SliceRefIterator<Ptr>, Ptr>;
}

#[docs(no_index)]
mod internal {
    use builtins::{Primitive, Pointer, RangeOf};

    #[allow(unused_parameter)]
    macro bounds_check($cond, $msg, $args...) {
        #[cfg(any(debug, bounds_checks))]
        {
            if !$cond {
                panic!($msg, $args...);
            }
        }
    }

    // Some gnarly metaprogramming to determine whether collection-to-slice coercion is possible,
    // what the slice element type is, and with what mutability.
    type slicify_element_t<T> = typeof(*(null as &T).as_slice()._ptr);
    type slicify_slice_t<T: AsSlice<T, slicify_element_t<T>>, Ptr: builtins::PointerOf<T>> = when typing::matches::<Ptr, &T>() {
        &[slicify_element_t<T>]
    } else when typing::matches::<T, AsSliceMut<T, slicify_element_t<T>>>() {
        &mut [slicify_element_t<T>]
    } else {
        &[slicify_element_t<T>]
    };

    /// Convert anything that has the appropriate as_slice/as_slice_mut methods to a slice.
    /// Invoked by the compiler when indexing into a collection.
    #[inline(ir)]
    #[lang(slice_slicify)]
    fn slice_slicify<T, Ptr>(ptr: Ptr) -> slicify_slice_t<T, Ptr> {
        when ptr is &T {
            ptr.as_slice()
        } else when typing::matches::<T, AsSliceMut<T, slicify_element_t<T>>>() {
            ptr.as_slice_mut()
        } else {
            ptr.as_slice()
        }
    }

    /// Implementation for single-element indexing for slices.
    ///
    /// Compiler will convert `slice[i]` to `*slice_index(slice, i)`.
    /// Slice indexing is bounds-checked in debug mode.
    #[lang(slice_index)]
    #[cfg_attr(any(debug, bounds_checks), inline(always))]
    #[cfg_attr(not(any(debug, bounds_checks)), inline(ir))]
    fn slice_index<Ptr: Pointer>(a: slice<Ptr>, idx: usize) -> Ptr {
        bounds_check!(idx < a.len(), "index out of bounds: the len is {} but the index is {}", a.len(), idx);

        a._ptr + idx
    }

    /// Implementation for range indexing for slices.
    ///
    /// Compiler will convert `slice[ran..ge]` to `slice_index(slice, ran..ge)`.
    /// It is generic over all range types (`..`, `ran..`, `..ge` and `ran..ge`)
    #[lang(slice_range_index)]
    #[cfg_attr(any(debug, bounds_checks), inline(always))]
    #[cfg_attr(not(any(debug, bounds_checks)), inline(ir))]
    fn slice_range_index<Ptr: Pointer, T: RangeOf<usize>>(a: slice<Ptr>, range: T) -> slice<Ptr> {
        when range is range::Range<usize> {
            bounds_check!(
                range.lower <= range.upper,
                "index out of bounds: lower bound {} is greater than upper bound {}",
                range.lower,
                range.upper
            );
            bounds_check!(
                range.upper <= a.len(),
                "index out of bounds: the len is {} but the upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, range.upper - range.lower)
        } else when range is range::RangeInclusive<usize> {
            bounds_check!(
                range.lower <= range.upper,
                "index out of bounds: lower bound {} is greater than upper bound {}",
                range.lower,
                range.upper
            );
            bounds_check!(
                range.upper < a.len(),
                "index out of bounds: the len is {} but the inclusive upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, range.upper - range.lower + 1)
        } else when range is range::RangeTo<usize> {
            bounds_check!(
                range.upper <= a.len(),
                "index out of bounds: the len is {} but the upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr, range.upper)
        } else when range is range::RangeToInclusive<usize> {
            bounds_check!(
                range.upper < a.len(),
                "index out of bounds: the len is {} but the inclusive upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr, range.upper + 1)
        } else when range is range::RangeFrom<usize> {
            bounds_check!(
                range.lower <= a.len(),
                "index out of bounds: the len is {} but the lower bound is {}",
                a.len(),
                range.lower
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, a.len() - range.lower)
        } else when range is range::RangeFull<usize> {
            a
        } else {
            compile_fail!("unsupported range type");
        }
    }

    /// Compiler invokes this function to coerce a mutable slice
    /// into a const slice.
    ///
    /// This is necessary, since they are completely
    /// different types.
    #[inline(ir)]
    #[lang(slice_const_coerce)]
    fn slice_const_coerce<T>(a: slice<&mut T>) -> slice<&T> {
        slice { _ptr: a._ptr as &T, _len: a._len }
    }

    #[inline(ir)]
    #[lang(slice_const_cast)]
    fn slice_const_cast<T>(a: slice<&T>) -> slice<&mut T> {
        slice { _ptr: a._ptr as &mut T, _len: a._len }
    }
}

/// Allocates a single object on the heap using a default allocator (`malloc`)
///
/// The value is not initialized. See [alloc_zeroed] which returns zeroed memory.
///
/// See also a [the matching method](slice::alloc) for allocating a slice.
///
/// ## Example
/// ```
/// use std::mem::{alloc, free};
///
/// let x: &mut i32 = alloc();
/// *x = 42;
/// assert_eq!(*x, 42);
/// free(x);
/// ```
fn alloc<T>() -> &mut T {
    when typing::is_zero_sized::<T>() {
        dangling()
    } else {
        libc::malloc(size_of::<T>()) as &mut T
    }
}

/// Allocates a single object on the heap using a default allocator (`malloc`)
///
/// The value is zero-initialized.
///
/// See also a [the matching method](slice::alloc_zeroed) for allocating a slice.
///
/// ## Example
/// ```
/// use std::mem::{alloc_zeroed, free};
///
/// let x: &mut i32 = alloc_zeroed();
/// assert_eq!(*x, 0);
/// free(x);
/// ```
fn alloc_zeroed<T>() -> &mut T {
    when typing::is_zero_sized::<T>() {
        dangling()
    } else {
        libc::calloc(1, size_of::<T>()) as &mut T
    }
}

/// Allocates an array of specified size on the stack.
///
/// The values are not initialized. The array is allocated on the stack, so it will be
/// deallocated when the function returns.
///
/// Care must be taken to ensure that the array is not accessed after the function returns.
/// Additionally, there is no protection against stack overflow, so allocated arrays should
/// not be too large.
///
/// ## Example
/// ```
/// use std::mem::stack_alloc;
///
/// let hw = "Hello, World!";
/// let s = stack_alloc::<u8>(hw.len());
///
/// hw.copy_to(&s[0]);
/// assert_eq!(hw, s);
/// ```
///
/// ```nasaldemons
/// use std::mem::stack_alloc;
///
/// stack_alloc::<i32>(10000000); // stack overflow, probably
/// ```
#[inline(ir)]
fn stack_alloc<T>(len: usize) -> &mut [T] {
    slice::from_raw(when typing::is_zero_sized::<T>() {
        dangling::<&mut T>()
    } else {
        // Cannot use __builtin_alloca_with_align as it may not be valid until the end of the function
        intrinsics::codegen_func::<&mut void>(
            "__builtin_alloca",
            size_of::<T>() * len
        ) as &mut T
    }, len)
}

/// Frees a heap-allocated object
///
/// The pointer must be allocated with the default allocator (`malloc`).
///
/// ## Example
/// ```
/// struct Box<T> { ptr: &mut T }
///
/// impl Box<T> {
///     fn new(value: T) -> Box<T> {
///         let ptr = std::mem::alloc::<T>();
///         *ptr = value;
///         Box { ptr: ptr }
///     }
///
///     fn free(self: &mut Box<T>) {
///         std::mem::free(self.ptr);
///     }
/// }
/// ```
fn free<T>(a: &mut T) {
    when !(*a is builtins::ZeroSized) {
        libc::free(a as &mut void);
    }
}

/// Memory size of for a given type in bytes.
///
/// ## Example
/// ```
/// use std::mem::size_of;
///
/// assert_eq!(size_of::<u8>(), 1);
/// assert_eq!(size_of::<u16>(), 2);
/// assert_eq!(size_of::<u32>(), 4);
/// assert_eq!(size_of::<u64>(), 8);
/// ```
#[inline(ir)]
fn size_of<T>() -> usize {
    intrinsics::size_of::<T>()
}

/// Minimum alignment for given type in bytes.
///
/// ## Example
/// ```
/// use std::mem::align_of;
///
/// // On most platforms
/// assert_eq!(align_of::<u8>(), 1);
/// assert_eq!(align_of::<u16>(), 2);
/// assert_eq!(align_of::<u32>(), 4);
/// assert_eq!(align_of::<u64>(), 8);
/// ```
#[inline(ir)]
fn align_of<T>() -> usize {
    intrinsics::align_of::<T>()
}

/// Swaps the data in two memory locations.
///
/// ## Example
/// ```
/// use std::mem::swap;
///
/// let a = 1;
/// let b = 2;
///
/// swap(&a, &b);
///
/// assert_eq!(a, 2);
/// assert_eq!(b, 1);
/// ```
fn swap<T>(a: &mut T, b: &mut T) {
    let tmp = *a;
    *a = *b;
    *b = tmp;
}

/// Replaces a memory at location `a` with value `b`.
///
/// The existing value is returned.
///
/// ## Example
/// ```
/// use std::mem::replace;
///
/// let a = 1;
///
/// assert_eq!(a.replace(2), 1);
/// assert_eq!(a, 2);
/// ```
fn replace<T>(a: &mut T, b: T) -> T {
    swap(a, &b);
    b
}

/// Zero-initialized object of a given type.
///
/// ## Example
/// ```
/// use std::mem::zeroed;
///
/// struct Foo { a: u8, b: u16 }
///
/// let a: Foo = zeroed();
///
/// assert_eq!(a.a, 0);
/// assert_eq!(a.b, 0);
/// ```
fn zeroed<T>() -> T {
    let ret: T;
    when ret is bool {
        false
    } else when (ret is builtins::Primitive) && !(ret is builtins::ZeroSized) {
        ret = 0 as T;
    } else when !(ret is builtins::ZeroSized) {
        libc::memset(&ret as &mut void, 0, size_of::<T>());
    }
    ret
}

/// Uninitialized object.
///
/// Using the return value is usually undefined behavior.
///
/// ## Examples
///
/// This is probably fine:
///
/// ```
/// use std::mem::uninitialized;
///
/// struct MyOption<T> { some: bool, val: T }
///
/// let _: MyOption<i32> = MyOption { some: true, val: 2 };
/// let _: MyOption<i32> = MyOption { some: false, val: uninitialized() };
/// ```
///
/// This is not:
/// ```nasaldemons
/// use std::mem::uninitialized;
///
/// let val: i32 = uninitialized();
/// if val > 0 {  // UB!
///     println!("positive");
/// } else {
///     println!("negative");
/// }
/// ```
#[inline(ir)]
fn uninitialized<T>() -> T {
    intrinsics::uninitialized::<T>()
}

/// Copies a region of memory from `src` to `dst`.
///
/// The memory ranges must not overlap, use [copy] if they may be overlapping.
fn copy_nonoverlapping<T>(src: &T, dst: &mut T, count: usize) {
    when !(*src is builtins::ZeroSized) {
        libc::memcpy(dst as &mut void, src as &void, count * size_of::<T>());
    }
}

/// Copies a region of memory from `src` to `dst`.
///
/// The ranges may overlap.
fn copy<T>(src: &T, dst: &mut T, count: usize) {
    when !(*src is builtins::ZeroSized) {
        libc::memmove(dst as &mut void, src as &void, count * size_of::<T>());
    }
}

/// Returns a dangling non-null pointer.
///
/// The pointer is appropriately aligned for the provided type, and is non-null.
///
/// This can be used as a sentinel value for e.g. collections of zero-sized types or
/// empty slices where the pointer is never dereferenced, but `null` cannot be used
/// for whatever reason.
///
/// This is the pointer that is returned when taking an address of a value that has a
/// zero-sized type.
///
/// If the pointer points to a sized type, dereferencing it is undefined behavior. If
/// the pointer points to a zero-sized type, dereferencing it is no-op.
///
/// ## Example
/// ```
/// use std::mem::dangling;
///
/// struct Foo { a: (), b: [u64; 0] }
///
/// let foo: Foo;
///
/// assert_eq!(&foo.a, dangling::<&mut ()>());
/// assert_eq!(&foo.b, dangling::<&mut [u64; 0]>());
///
/// // Because the types have different alignment. Currently `dangling`
/// // just returns the alignment of the type (e.g. 1, 2, 4, ...)
/// // cast to the pointer type.
/// assert_ne!(&foo.a as &mut void, &foo.b as &mut void);
/// ```
#[inline(ir)]
fn dangling<Ptr: builtins::Pointer>() -> Ptr {
    intrinsics::dangling::<Ptr>()
}

#[cfg(all(test, test_std))]
#[docs(hide)]
mod tests {
    #[test]
    fn slice_range_index() {
        let a = [1, 2, 3, 4, 5].as_slice();

        assert_eq!(a[..], &[1, 2, 3, 4, 5]);
        assert_eq!(a[..3], &[1, 2, 3]);
        assert_eq!(a[1..], &[2, 3, 4, 5]);
        assert_eq!(a[1..5], &[2, 3, 4, 5]);

        assert_eq!(a[..=3], &[1, 2, 3, 4]);
        assert_eq!(a[1..=3], &[2, 3, 4]);
    }

    #[test]
    fn size_of_zst() {
        assert_eq!(size_of::<()>(), 0usize);
        assert_eq!(size_of::<[u8; 0]>(), 0usize);
        assert_eq!(size_of::<((), (), ())>(), 0usize);
    }

    #[test]
    fn align_of_zst() {
        assert_eq!(align_of::<()>(), 1usize);
        assert_eq!(align_of::<[u8; 0]>(), 1usize);
        assert_eq!(align_of::<((), (), ())>(), 1usize);
    }

    #[test]
    fn test_fill() {
        let a: [u8; 10];
        a[..].fill('a');

        assert_eq!(a[..] as &[u8], "aaaaaaaaaa");
    }

    #[test]
    fn test_copy_nonoverlapping() {
        let a: [u8; 32];
        "hello world".copy_to_nonoverlapping(&a[0]);

        assert_eq!(a[..11] as &[u8], "hello world");
    }

    #[test]
    fn test_copy() {
        let a: [u8; 32];

        "hello world".copy_to_nonoverlapping(&a[0]);
        a[0..11].copy_to(&a[6]);

        assert_eq!(a[0..17] as &[u8], "hello hello world");
    }

    #[test]
    fn test_slice_equals() {
        assert_eq!("hello", "hello");
        assert_ne!("hello", "hella");
        assert_ne!("hello", "hell");
        assert_ne!("hell", "hello");

        assert_eq!([1,2,3,4].as_slice(), [1,2,3,4].as_slice());
        assert_ne!([1,2,3,4].as_slice(), [1,2,3,5].as_slice());
        assert_ne!([1,2,3,4].as_slice(), [1,2,3].as_slice());
        assert_ne!([1,2,3].as_slice(), [1,2,3,4].as_slice());
    }

    #[test]
    fn test_slice_compare() {
        assert!("hello" as &[u8] > "hella" as &[u8]);
        assert!("hello" as &[u8] > "hell" as &[u8]);
        assert!("hell" as &[u8] < "hello" as &[u8]);

        assert!([1,2,3,4].as_slice() > [1,2,3,2].as_slice());
        assert!([1,2,3,4].as_slice() > [1,2,3].as_slice());
        assert!([1,2,3].as_slice() < [1,2,3,4].as_slice());
    }

    #[test]
    fn test_bounds_check() {
        let a = [1,2,3,4,5].as_slice();
        a[4];
        a[0..5];
        a[..5];
        a[5..];
        a[..=4];
        a[0..=4];
        a[0..=0];
        a[..=0];
    }

    #[cfg(debug)]
    {
        #[test(should_fail)]
        fn test_bounds_check_1() {
            let a = [1,2,3,4,5].as_slice();
            a[5];
        }

        #[test(should_fail)]
        fn test_bounds_check_2() {
            let a = [1,2,3,4,5].as_slice();
            a[4..3];
        }

        #[test(should_fail)]
        fn test_bounds_check_3() {
            let a = [1,2,3,4,5].as_slice();
            a[6..];
        }

        #[test(should_fail)]
        fn test_bounds_check_4() {
            let a = [1,2,3,4,5].as_slice();
            a[..6];
        }

        #[test(should_fail)]
        fn test_bounds_check_5() {
            let a = [1,2,3,4,5].as_slice();
            a[..=5];
        }

        #[test(should_fail)]
        fn test_bounds_check_6() {
            let a = [1,2,3,4,5].as_slice();
            a[0..=5];
        }
    }

    #[test]
    fn test_stack_alloc() {
        let a: &mut [i32] = stack_alloc(10);

        (0..10).fill_slice(a);

        assert_eq!(a, &[0,1,2,3,4,5,6,7,8,9]);
    }
}
