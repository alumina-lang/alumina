//! Working with memory (allocation, slices, copying, ...)
//!
//! The main feature of this module is the [slice] type, which is a pointer to a region
//! of memory. `&[T]` is a slice (read-only) of `T`, and `&mut [T]` is a mutable slice.

/// Types that can be "freed".
///
/// Usually these types contain something that is heap-allocated.
protocol Freeable<Self> {
    /// Frees the memory backing the object.
    fn free(self: &mut Self);
}

/// Types that implement move semantics
///
/// If a type allocates memory, it is a good idea to implement move function, so that instances
/// can be passed around by value, leaving inert instances behind. This is chiefly used to `defer`
/// expressions are more ergonomic.
///
/// ```
/// use std::collections::Vector;
///
/// fn do_something(v: Vector<i32>) {
///     // ...
///     v.free();
/// }
///
/// let foo: Vector<i32> = Vector::new();
/// defer foo.free(); // Clean up foo if function returns early
/// // ...
/// do_something(foo.move());
/// // will not lead to double free when this function returns.
/// ```
protocol Movable<Self> {
    /// Creates a copy of the object, leaving an inert object behind.
    ///
    /// See [Movable] for details.
    fn move(self: &mut Self) -> Self;
}

/// Types that be copied non-trivially (usually types containing a heap allocation).
protocol Clonable<Self> {
    /// Returns a copy of the object.
    fn clone(self: &Self) -> Self;
}

protocol Borrowable<Self, C> {
    /// Returns a view to the object.
    fn borrow(self: &Self) -> C;
}

protocol BorrowableMut<Self, M> {
    /// Returns a mutable view to the object.
    fn borrow_mut(self: &mut Self) -> M;
}

/// Types that can be viewed as const slices
///
/// Types that implement this protocol can be indexed as if they were slices.
///
/// ## Example
/// ```
/// use std::collections::Vector;
///
/// let v: Vector<i32> = Vector::new();
/// v.push(1);
/// v.push(2);
/// v.push(3);
///
/// assert_eq!(v[1], 2);
/// ```
protocol AsSlice<Self, T> {
    /// Returns the contents of the object as a slice.
    ///
    /// See [AsSlice] for details.
    fn as_slice(self: &Self) -> &[T];
}

/// Types that can be viewed as mutable slices
///
/// Types that implement this protocol can be indexed as if they were slices.
///
/// ## Example
/// ```
/// use std::collections::Vector;
///
/// let v: Vector<i32> = Vector::new();
/// v.push(1);
/// v.push(2);
/// v.push(3);
///
/// v[2] = 4;
///
/// assert_eq!(v.pop(), Option::some(4));
/// ```
protocol AsSliceMut<Self: AsSlice<Self, T>, T> {
    /// Returns the contents of the object as a mutable slice.
    ///
    /// See [AsSliceMut] for details.
    fn as_slice_mut(self: &mut Self) -> &mut [T];
}

/// Fat pointers to a contiguous region of memory.
///
/// ```pseudo_alumina
/// type &[T] = slice<&T>;
/// type &mut [T] = slice<&mut T>;
/// ```
///
/// Slice fat "pointers" are just regular structs that compiler handles in a special way with regards
/// to syntax, implicit coercion and type inference. They are generic over the pointer-to-element type
/// rather than the element type itself. This is an implementation detail to ensure that `&mut [T]` and
/// `&[T]` are distinguished without having to have two distinct types for mutable and const slices.
#[lang(slice)]
struct slice<Ptr: builtins::Pointer> {
    _ptr: Ptr,
    _len: usize,
}

impl slice {
    use builtins::Pointer;
    use cmp::{Equatable, Comparable, Ordering};
    use hash::{Hasher, Hashable};
    use builtins::{Primitive, PointerOf, ZeroSized};
    use libc::memcmp;

    /// Empty slice
    #[inline(always)]
    fn empty<Ptr: Pointer>() -> slice<Ptr> {
        slice::<Ptr> { _ptr: dangling::<Ptr>(), _len: 0 }
    }

    /// Create a slice from a pointer and length
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let arr = [1, 2, 3];
    /// let ptr: &i32 = &arr[0];
    ///
    /// let slice = slice::from_raw(ptr, 2);
    /// assert_eq!(slice, &[1, 2]);
    /// ```
    #[inline(ir)]
    #[lang(slice_new)]
    fn from_raw<Ptr: Pointer>(ptr: Ptr, len: usize) -> slice<Ptr> {
        slice::<Ptr> { _ptr: ptr, _len: len }
    }

    /// Returns the length of the slice
    #[inline(ir)]
    fn len<Ptr: Pointer>(self: slice<Ptr>) -> usize {
        self._len
    }

    /// Returns a pointer to the first element of the slice.
    ///
    /// Unlike `&slice[0]`, this is not range-checked, so it will not panic
    /// if the slice is empty, but may return `null`, a dangling or an otherwise
    /// invalid pointer.
    #[inline(always)]
    fn as_ptr<Ptr: Pointer>(self: slice<Ptr>) -> Ptr {
        self._ptr
    }

    /// Returns `true` if the slice has a length of 0, and `false` otherwise.
    #[inline]
    fn is_empty<Ptr: Pointer>(self: slice<Ptr>) -> bool {
        self._len == 0
    }

    /// Allocates an array of specified size on the heap.
    ///
    /// The values are not initialized.
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let hw = "Hello, World!";
    /// let s = slice::alloc::<u8>(hw.len());
    /// defer s.free();
    ///
    /// hw.copy_to(&s[0]);
    /// assert_eq!(hw, s);
    /// ```
    fn alloc<T>(len: usize) -> &mut [T] {
        let ptr = when typing::is_zero_sized::<T>() {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            intrinsics::const_alloc::<T>(len)
        } else {
            libc::malloc(size_of::<T>() * len) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Allocates an array of specified size on the heap.
    ///
    /// The values are zero-initialized.
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let s = slice::alloc_zeroed::<u8>(10);
    /// defer s.free();
    ///
    /// assert_eq!(s, &[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]);
    /// ```
    fn alloc_zeroed<T>(len: usize) -> &mut [T] {
        let ptr = when typing::matches::<T, builtins::ZeroSized>() {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            let ptr = intrinsics::const_alloc::<T>(len);
            for i in 0usize..len {
                *(ptr + i) = zeroed::<T>();
            }
            ptr
        } else {
            libc::calloc(len, size_of::<T>()) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Resizes a heap-allocated array. The original array must have been
    /// allocated with [slice::alloc].
    ///
    /// If the new size is larger than the old size, the new elements are
    /// uninitialized.
    ///
    /// ## Example
    /// ```
    /// use std::mem::slice;
    ///
    /// let v = slice::alloc::<i32>(2);
    /// defer v.free();
    ///
    /// v[0] = 1;
    /// v[1] = 2;
    ///
    /// v = v.realloc(3);
    /// v[2] = 3;
    ///
    /// assert_eq!(v, &[1, 2, 3]);
    /// ```
    fn realloc<T>(slice: &mut [T], len: usize) -> &mut [T] {
        let ptr = when slice[0] is builtins::ZeroSized {
            dangling::<&mut T>()
        } else if runtime::in_const_context() {
            let new = intrinsics::const_alloc::<T>(len);
            slice.copy_to_nonoverlapping(new);
            slice.free();
            new
        } else {
            libc::realloc(slice._ptr as &mut void, size_of::<T>() * len) as &mut T
        };

        from_raw(ptr, len)
    }

    /// Copies a region of memory from `src` to `dst`.
    ///
    /// The memory regions must not overlap, use [copy_to] if they may
    /// be overlapping.
    ///
    /// ## Example
    /// ```
    /// let src = [1, 2, 3];
    /// let dst: [i32; 3];
    ///
    /// src[..].copy_to_nonoverlapping(&dst[0]);
    /// assert_eq!(dst, [1, 2, 3]);
    /// ```
    fn copy_to_nonoverlapping<T>(src: &[T], dst: &mut T) {
        copy_nonoverlapping(src._ptr, dst, src.len());
    }

    /// Copies a region of memory from `src` to `dst`.
    ///
    /// The regions may overlap. Using [copy_to_nonoverlapping] can be more
    /// efficient if the ranges do not in fact overlap.
    ///
    /// ## Example
    /// ```
    /// let src = [1, 2, 3, 4, 5, 6];
    ///
    /// src[..3].copy_to(&src[1]);
    /// src[4..].copy_to(&src[3]);
    /// assert_eq!(src, [1, 1, 2, 5, 6, 6]);
    /// ```
    fn copy_to<T>(src: &[T], dst: &mut T) {
        copy(src._ptr, dst, src.len());
    }

    /// Returns the element at the given index.
    ///
    /// If the index is out of bounds, the function will return `Option::none()`.
    ///
    /// ## Example
    /// ```
    /// let hw = "Hello, world";
    /// assert_eq!(hw[..5].get(0), Option::some('H'));
    /// assert_eq!(hw[..5].get(5), Option::none());
    /// ```
    fn get<T>(self: &[T], index: usize) -> Option<T> {
        if index < self.len() {
            Option::some(*(self._ptr + index))
        } else {
            Option::none()
        }
    }

    /// Returns a subslice of the given slice.
    ///
    /// This is a safe version of range indexing (`slice[a..b]`, `slice[a..]`, ...). Instead
    /// of panicking or causing UB when the range is out of bounds, this function returns
    /// `Option::none()`.
    ///
    /// ## Example
    /// ```
    /// let hw = "Hello, world";
    /// assert_eq!(hw.get_range(..5usize), Option::some("Hello"));
    /// assert_eq!(hw.get_range(7usize..), Option::some("world"));
    /// assert_eq!(hw.get_range(7usize..12), Option::some("world"));
    /// assert_eq!(hw.get_range(7usize..13), Option::none());
    /// ```
    fn get_range<Ptr: Pointer, T: builtins::RangeOf<usize>>(self: slice<Ptr>, range: T) -> Option<slice<Ptr>> {
        when range is range::Range<usize> {
            if (range.lower <= range.upper && range.upper <= self.len()) {
                Option::some(slice::from_raw::<Ptr>(self._ptr + range.lower, range.upper - range.lower))
            } else {
                Option::none()
            }
        } else when range is range::RangeInclusive<usize> {
            if (range.lower <= range.upper && range.upper < self.len()) {
                Option::some(slice::from_raw::<Ptr>(self._ptr + range.lower, range.upper - range.lower + 1))
            } else {
                Option::none()
            }
        } else when range is range::RangeTo<usize> {
            if (range.upper <= self.len()) {
                Option::some(slice::from_raw::<Ptr>(self._ptr, range.upper))
            } else {
                Option::none()
            }
        } else when range is range::RangeToInclusive<usize> {
            if (range.upper < self.len()) {
                Option::some(slice::from_raw::<Ptr>(self._ptr, range.upper + 1))
            } else {
                Option::none()
            }
        } else when range is range::RangeFrom<usize> {
            if (range.lower <= self.len()) {
                Option::some(slice::from_raw::<Ptr>(self._ptr + range.lower, self.len() - range.lower))
            } else {
                Option::none()
            }
        } else when range is range::RangeFull<usize> {
            Option::some(self)
        } else {
            compile_fail!("unsupported range type");
        }
    }

    /// Copy a slice into a fixed-size array.
    ///
    /// The length of the result array must exactly match the length of the slice.
    ///
    /// ## Example
    /// ```
    /// let hw = "Hello, world";
    /// let hw: [u8; 5] = hw[..5].to_array();
    ///
    /// assert_eq!(hw[..] as &[u8], "Hello");
    /// ```
    fn to_array<T, Arr: builtins::ArrayOf<T>>(self: &[T]) -> Arr {
        let ret: Arr;
        debug_assert!(self.len() == ret.len());
        self.copy_to_nonoverlapping(&ret[0]);
        ret
    }

    /// @ std::iter::Iterable::iter
    #[inline(always)]
    fn iter<Ptr: Pointer>(self: slice<Ptr>) -> SliceIterator<Ptr> {
        SliceIterator { inner: self }
    }

    /// @ std::iter::IterableRef::iter_ref
    #[inline(always)]
    fn iter_ref<Ptr: Pointer>(self: slice<Ptr>) -> SliceRefIterator<&*Ptr> {
        SliceRefIterator { inner: self as slice<&*Ptr> }
    }

    /// @ std::iter::IterableMut::iter_mut
    #[inline(always)]
    fn iter_mut<T>(self: &mut [T]) -> SliceRefIterator<&mut T> {
        SliceRefIterator { inner: self }
    }

    /// Fill slice with a value
    ///
    /// ## Example
    /// ```
    /// let v = [0, 0, 0];
    /// v[..].fill(1);
    /// assert_eq!(v, [1, 1, 1]);
    /// ```
    fn fill<T>(slice: &mut [T], value: T) {
        when slice[0] is ZeroSized {
            // nop
        } else when slice[0] is u8 {
            if runtime::in_const_context() {
                for i in 0usize..slice.len() {
                    slice[i] = value;
                }
            } else {
                libc::memset(slice._ptr as &mut void, value as libc::c_int, slice.len());
            }
        } else {
            for i in 0usize..slice.len() {
                slice[i] = value;
            }
        }
    }

    /// @ std::cmp::Equatable::equals
    fn equals<T: Equatable<T>, Ptr: PointerOf<T>>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> bool {
        when *lhs._ptr is ZeroSized {
            lhs.len() == rhs.len()
        } else when *lhs._ptr is Primitive {
            // Optimization for slices of primitive types
            if runtime::in_const_context() {
                internal::default_equals::<Ptr>(lhs, rhs)
            } else {
                lhs.len() == rhs.len() &&
                    (memcmp(lhs._ptr as &void, rhs._ptr as &void, lhs.len() * size_of::<T>()) == 0)
            }
        } else {
            internal::default_equals::<Ptr>(lhs, rhs)
        }
    }

    /// @ std::cmp::Comparable::compare
    fn compare<T: Comparable<T>, Ptr: PointerOf<T>>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> Ordering {
        use cmp::min;

        when *lhs._ptr is ZeroSized {
            lhs.len().compare(&rhs.len())
        } else when *lhs._ptr is u8 {
            if runtime::in_const_context() {
                internal::default_compare::<Ptr>(lhs, rhs)
            } else {
                let cmp = memcmp(lhs._ptr as &void, rhs._ptr as &void, min(lhs.len(), rhs.len()) * size_of::<T>());
                if cmp < 0 {
                    Ordering::Less
                } else if cmp > 0 {
                    Ordering::Greater
                } else {
                    lhs.len().compare(&rhs.len())
                }
            }
        } else {
            internal::default_compare::<Ptr>(lhs, rhs)
        }
    }

    /// @ std::hash::Hashable::hash
    fn hash<T: Hashable<T, H>, Ptr: PointerOf<T>, H: Hasher<H>>(self: &slice<Ptr>, hasher: &mut H) {
        when *self._ptr is ZeroSized {
            // no-op
        } else when *self._ptr is u8 {
            hasher.write(*self);
        } else {
            let idx = 0usize;
            while idx < self.len() {
                (*self)[idx].hash(hasher);
                idx += 1;
            }
        }
    }

    /// @ std::fmt::Formattable::fmt
    fn fmt<Ptr: PointerOf<u8>, F: fmt::Formatter<F>>(self: &slice<Ptr>, f: &mut F) -> Result<(), fmt::Error> {
        f.write_str(*self)
    }

    /// Frees the slice.
    ///
    /// This requires that the slice was allocated with `alloc` or `alloc_zeroed`.
    fn free<T>(a: &mut [T]) {
        when !(*a._ptr is builtins::ZeroSized) {
            if runtime::in_const_context() {
                intrinsics::const_free::<T>(a._ptr);
            } else {
                libc::free(a._ptr as &mut void);
            }
        }
    }

    mixin<T: Equatable<T>, Ptr: PointerOf<T>> Equatable<slice<Ptr>>;
    mixin<T: Comparable<T>, Ptr: PointerOf<T>> Comparable<slice<Ptr>>;
}

/// Iterator over elements of a slice
struct SliceIterator<Ptr: builtins::Pointer> {
    inner: slice<Ptr>,
}

impl SliceIterator<Ptr: builtins::Pointer> {
    /// @ iter::Iterator::next
    #[inline(always)]
    fn next(self: &mut SliceIterator<Ptr>) -> Option<*Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(*self.inner._ptr);
            self.inner._ptr = self.inner._ptr + 1;
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::DoubleEndedIterator::next_back
    #[inline(always)]
    fn next_back(self: &mut SliceIterator<Ptr>) -> Option<*Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(self.inner[self.inner.len() - 1]);
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::Iterator::size_hint
    fn size_hint(self: &SliceIterator<Ptr>) -> Option<usize> {
        Option::some(self.inner.len())
    }

    mixin iter::Iterator<SliceIterator<Ptr>, *Ptr>;
    mixin iter::IteratorExt<SliceIterator<Ptr>, *Ptr>;
    mixin iter::DoubleEndedIterator<SliceIterator<Ptr>, *Ptr>;
    mixin iter::DoubleEndedIteratorExt<SliceIterator<Ptr>, *Ptr>;
}

/// Iterator over pointers to elements of a slice
struct SliceRefIterator<Ptr: builtins::Pointer> {
    inner: slice<Ptr>,
}

impl SliceRefIterator<Ptr: builtins::Pointer> {
    /// @ iter::Iterator::next
    #[inline(always)]
    fn next(self: &mut SliceRefIterator<Ptr>) -> Option<Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(self.inner._ptr);
            self.inner._ptr = self.inner._ptr + 1;
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::DoubleEndedIterator::next_back
    #[inline(always)]
    fn next_back(self: &mut SliceRefIterator<Ptr>) -> Option<Ptr> {
        if self.inner.len() > 0 {
            let result = Option::some(&self.inner[self.inner.len() - 1]);
            self.inner._len -= 1;
            result
        } else {
            Option::none()
        }
    }

    /// @ iter::Iterator::size_hint
    fn size_hint(self: &SliceRefIterator<Ptr>) -> Option<usize> {
        Option::some(self.inner.len())
    }

    mixin iter::Iterator<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::IteratorExt<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::DoubleEndedIterator<SliceRefIterator<Ptr>, Ptr>;
    mixin iter::DoubleEndedIteratorExt<SliceRefIterator<Ptr>, Ptr>;
}

#[docs(no_index)]
mod internal {
    use builtins::{Primitive, Pointer, RangeOf};

    #[allow(unused_parameter)]
    macro bounds_check($cond, $msg, $args...) {
        #[cfg(any(debug, bounds_checks))]
        {
            if !$cond {
                panic!($msg, $args$...);
            }
        }
    }

    // Some gnarly metaprogramming to determine whether collection-to-slice coercion is possible,
    // what the slice element type is, and with what mutability.
    type slicify_element_t<T> = typeof(*(null as &T).as_slice()._ptr);
    type slicify_slice_t<T: AsSlice<T, slicify_element_t<T>>, Ptr: builtins::PointerOf<T>> = when typing::matches::<Ptr, &T>() {
        &[slicify_element_t<T>]
    } else when typing::matches::<T, AsSliceMut<T, slicify_element_t<T>>>() {
        &mut [slicify_element_t<T>]
    } else {
        &[slicify_element_t<T>]
    };

    /// Convert anything that has the appropriate as_slice/as_slice_mut methods to a slice.
    /// Invoked by the compiler when indexing into a collection.
    #[inline(ir)]
    #[lang(slice_slicify)]
    fn slice_slicify<T, Ptr>(ptr: Ptr) -> slicify_slice_t<T, Ptr> {
        when ptr is &T {
            ptr.as_slice()
        } else when typing::matches::<T, AsSliceMut<T, slicify_element_t<T>>>() {
            ptr.as_slice_mut()
        } else {
            ptr.as_slice()
        }
    }

    /// Implementation for single-element indexing for slices.
    ///
    /// Compiler will convert `slice[i]` to `*slice_index(slice, i)`.
    /// Slice indexing is bounds-checked in debug mode.
    #[lang(slice_index)]
    #[cfg_attr(any(debug, bounds_checks), inline(always))]
    #[cfg_attr(not(any(debug, bounds_checks)), inline(ir))]
    fn slice_index<Ptr: Pointer>(a: slice<Ptr>, idx: usize) -> Ptr {
        bounds_check!(idx < a.len(), "index out of bounds: the len is {} but the index is {}", a.len(), idx);

        a._ptr + idx
    }

    /// Implementation for range indexing for slices.
    ///
    /// Compiler will convert `slice[ran..ge]` to `slice_index(slice, ran..ge)`.
    /// It is generic over all range types (`..`, `ran..`, `..ge` and `ran..ge`)
    #[lang(slice_range_index)]
    #[cfg_attr(any(debug, bounds_checks), inline(always))]
    #[cfg_attr(not(any(debug, bounds_checks)), inline(ir))]
    fn slice_range_index<Ptr: Pointer, T: RangeOf<usize>>(a: slice<Ptr>, range: T) -> slice<Ptr> {
        when range is range::Range<usize> {
            bounds_check!(
                range.lower <= range.upper,
                "index out of bounds: lower bound {} is greater than upper bound {}",
                range.lower,
                range.upper
            );
            bounds_check!(
                range.upper <= a.len(),
                "index out of bounds: the len is {} but the upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, range.upper - range.lower)
        } else when range is range::RangeInclusive<usize> {
            bounds_check!(
                range.lower <= range.upper,
                "index out of bounds: lower bound {} is greater than upper bound {}",
                range.lower,
                range.upper
            );
            bounds_check!(
                range.upper < a.len(),
                "index out of bounds: the len is {} but the inclusive upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, range.upper - range.lower + 1)
        } else when range is range::RangeTo<usize> {
            bounds_check!(
                range.upper <= a.len(),
                "index out of bounds: the len is {} but the upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr, range.upper)
        } else when range is range::RangeToInclusive<usize> {
            bounds_check!(
                range.upper < a.len(),
                "index out of bounds: the len is {} but the inclusive upper bound is {}",
                a.len(),
                range.upper
            );

            slice::from_raw::<Ptr>(a._ptr, range.upper + 1)
        } else when range is range::RangeFrom<usize> {
            bounds_check!(
                range.lower <= a.len(),
                "index out of bounds: the len is {} but the lower bound is {}",
                a.len(),
                range.lower
            );

            slice::from_raw::<Ptr>(a._ptr + range.lower, a.len() - range.lower)
        } else when range is range::RangeFull<usize> {
            a
        } else {
            compile_fail!("unsupported range type");
        }
    }

    /// Compiler invokes this function to coerce a mutable slice
    /// into a const slice.
    ///
    /// This is necessary, since they are completely
    /// different types.
    #[inline(ir)]
    #[lang(slice_const_coerce)]
    fn slice_const_coerce<T>(a: slice<&mut T>) -> slice<&T> {
        slice { _ptr: a._ptr as &T, _len: a._len }
    }

    #[inline(ir)]
    #[lang(slice_const_cast)]
    fn slice_const_cast<T>(a: slice<&T>) -> slice<&mut T> {
        slice { _ptr: a._ptr as &mut T, _len: a._len }
    }

    fn default_equals<Ptr>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> bool {
        if lhs._len != rhs._len {
            return false;
        }

        let idx = 0usize;
        while idx < lhs._len {
            if *(lhs._ptr + idx) != *(rhs._ptr + idx) {
                return false;
            }
            idx += 1;
        }
        true
    }

    fn default_compare<Ptr>(lhs: &slice<Ptr>, rhs: &slice<Ptr>) -> cmp::Ordering {
        let idx = 0usize;
        let len = cmp::min(lhs._len, rhs._len);
        while idx < len {
            let cmp = (*(lhs._ptr + idx)).compare(rhs._ptr + idx);
            if cmp != cmp::Ordering::Equal {
                return cmp;
            }
            idx += 1;
        }
        lhs._len.compare(&rhs._len)
    }
}

/// Allocates a single object on the heap using a default allocator (`malloc`)
///
/// The value is not initialized. See [alloc_zeroed] which returns zeroed memory.
///
/// See also a [the matching method](slice::alloc) for allocating a slice.
///
/// ## Example
/// ```
/// use std::mem::{alloc, free};
///
/// let x: &mut i32 = alloc();
/// *x = 42;
/// assert_eq!(*x, 42);
/// free(x);
/// ```
fn alloc<T>() -> &mut T {
    when typing::is_zero_sized::<T>() {
        dangling()
    } else if runtime::in_const_context() {
        intrinsics::const_alloc::<T>(1usize)
    } else {
        libc::malloc(size_of::<T>()) as &mut T
    }
}

/// Allocates a single object on the heap using a default allocator (`malloc`)
///
/// The value is zero-initialized.
///
/// See also a [the matching method](slice::alloc_zeroed) for allocating a slice.
///
/// ## Example
/// ```
/// use std::mem::{alloc_zeroed, free};
///
/// let x: &mut i32 = alloc_zeroed();
/// assert_eq!(*x, 0);
/// free(x);
/// ```
fn alloc_zeroed<T>() -> &mut T {
    when typing::is_zero_sized::<T>() {
        dangling()
    } else if runtime::in_const_context() {
        let ret = intrinsics::const_alloc::<T>(1usize);
        *ret = zeroed::<T>();
        ret
    } else {
        libc::calloc(1, size_of::<T>()) as &mut T
    }
}

/// Allocates an array of specified size on the stack.
///
/// The values are not initialized. The array is allocated on the stack, so it will be
/// deallocated when the function returns.
///
/// Care must be taken to ensure that the array is not accessed after the function returns.
/// Additionally, there is no protection against stack overflow, so allocated arrays should
/// not be too large.
///
/// ## Example
/// ```
/// use std::mem::stack_alloc;
///
/// let hw = "Hello, World!";
/// let s = stack_alloc::<u8>(hw.len());
///
/// hw.copy_to(&s[0]);
/// assert_eq!(hw, s);
/// ```
///
/// ```nasaldemons
/// use std::mem::stack_alloc;
///
/// stack_alloc::<i32>(10000000); // stack overflow, probably
/// ```
#[inline(ir)]
fn stack_alloc<T>(len: usize) -> &mut [T] {
    slice::from_raw(when typing::is_zero_sized::<T>() {
        dangling::<&mut T>()
    } else if runtime::in_const_context() {
        intrinsics::const_alloc::<T>(len)
    } else {
        // Cannot use __builtin_alloca_with_align as it may not be valid until the end of the function
        intrinsics::codegen_func::<&mut void>(
            "__builtin_alloca",
            size_of::<T>() * len
        ) as &mut T
    }, len)
}

/// Frees a heap-allocated object
///
/// The pointer must be allocated with the default allocator (`malloc`).
///
/// ## Example
/// ```
/// struct Box<T> { ptr: &mut T }
///
/// impl Box<T> {
///     fn new(value: T) -> Box<T> {
///         let ptr = std::mem::alloc::<T>();
///         *ptr = value;
///         Box { ptr: ptr }
///     }
///
///     fn free(self: &mut Box<T>) {
///         std::mem::free(self.ptr);
///     }
/// }
/// ```
fn free<T>(a: &mut T) {
    when !(*a is builtins::ZeroSized) {
        if runtime::in_const_context() {
            intrinsics::const_free::<T>(a);
        } else {
            libc::free(a as &mut void);
        }
    }
}

/// Memory size of for a given type in bytes.
///
/// ## Example
/// ```
/// use std::mem::size_of;
///
/// assert_eq!(size_of::<u8>(), 1);
/// assert_eq!(size_of::<u16>(), 2);
/// assert_eq!(size_of::<u32>(), 4);
/// assert_eq!(size_of::<u64>(), 8);
/// ```
#[inline(ir)]
fn size_of<T>() -> usize {
    intrinsics::size_of::<T>()
}

/// Minimum alignment for given type in bytes.
///
/// ## Example
/// ```
/// use std::mem::align_of;
///
/// // On most platforms
/// assert_eq!(align_of::<u8>(), 1);
/// assert_eq!(align_of::<u16>(), 2);
/// assert_eq!(align_of::<u32>(), 4);
/// assert_eq!(align_of::<u64>(), 8);
/// ```
#[inline(ir)]
fn align_of<T>() -> usize {
    intrinsics::align_of::<T>()
}

/// Swaps the data in two memory locations.
///
/// ## Example
/// ```
/// use std::mem::swap;
///
/// let a = 1;
/// let b = 2;
///
/// swap(&a, &b);
///
/// assert_eq!(a, 2);
/// assert_eq!(b, 1);
/// ```
fn swap<T>(a: &mut T, b: &mut T) {
    let tmp = *a;
    *a = *b;
    *b = tmp;
}

/// Replaces a memory at location `a` with value `b`.
///
/// The existing value is returned.
///
/// ## Example
/// ```
/// use std::mem::replace;
///
/// let a = 1;
///
/// assert_eq!(a.replace(2), 1);
/// assert_eq!(a, 2);
/// ```
fn replace<T>(a: &mut T, b: T) -> T {
    swap(a, &b);
    b
}

/// Zero-initialized object of a given type.
///
/// ## Example
/// ```
/// use std::mem::zeroed;
///
/// struct Foo { a: u8, b: u16 }
///
/// let a: Foo = zeroed();
///
/// assert_eq!(a.a, 0);
/// assert_eq!(a.b, 0);
/// ```
#[inline(ir)]
fn zeroed<T>() -> T {
    intrinsics::zeroed::<T>()
}

/// Uninitialized object.
///
/// Using the return value is usually undefined behavior.
///
/// ## Examples
///
/// This is probably fine:
///
/// ```
/// use std::mem::uninitialized;
///
/// struct MyOption<T> { some: bool, val: T }
///
/// let _: MyOption<i32> = MyOption { some: true, val: 2 };
/// let _: MyOption<i32> = MyOption { some: false, val: uninitialized() };
/// ```
///
/// This is not:
/// ```nasaldemons
/// use std::mem::uninitialized;
///
/// let val: i32 = uninitialized();
/// if val > 0 {  // UB!
///     println!("positive");
/// } else {
///     println!("negative");
/// }
/// ```
#[inline(ir)]
fn uninitialized<T>() -> T {
    intrinsics::uninitialized::<T>()
}

/// Copies a region of memory from `src` to `dst`.
///
/// The memory ranges must not overlap, use [copy] if they may be overlapping.
fn copy_nonoverlapping<T>(src: &T, dst: &mut T, count: usize) {
    when !(*src is builtins::ZeroSized) {
        if runtime::in_const_context() {
            for i in 0usize..count {
                *(dst + i) = *(src + i);
            }
        } else {
            libc::memcpy(dst as &mut void, src as &void, count * size_of::<T>());
        }
    }
}

/// Copies a region of memory from `src` to `dst`.
///
/// The ranges may overlap.
fn copy<T>(src: &T, dst: &mut T, count: usize) {
    when !(*src is builtins::ZeroSized) {
        if runtime::in_const_context() {
            // Not super efficient, but it doesn't matter since this is only used in
            // const contexts (which are very slow anyway).
            let tmp = intrinsics::const_alloc::<T>(count);
            copy_nonoverlapping(src, tmp, count);
            copy_nonoverlapping(tmp, dst, count);
            intrinsics::const_free::<T>(tmp);
        } else {
            libc::memmove(dst as &mut void, src as &void, count * size_of::<T>());
        }
    }
}

/// Performs a volatile read from a memory location.
///
/// This is useful in limited situations such as when dealing with signals/interrupts
/// and is not likely to do what you'd expect in a multi-threaded scenario. Use [sync::Atomic] instead.
///
/// When pointed-to type is zero-sized, no read is performed.
///
/// ## Example
/// ```no_run
/// use std::mem::{write_volatile, read_volatile};
///
/// static FLAG: i32;
///
/// fn signal_handler() {
///     FLAG.write_volatile(1);
/// }
///
/// while FLAG.read_volatile() == 0 {
///     // wait for something to set the flag
/// }
/// ```
#[inline(ir)]
fn read_volatile<T>(ptr: &T) -> T {
    when *ptr is builtins::ZeroSized {
        util::unit()
    } else {
        *intrinsics::volatile(ptr)
    }
}

/// Performs a volatile write to a memory location.
///
/// This is useful in limited situations such as when dealing with signals/interrupts
/// and is not likely to do what you'd expect in a multi-threaded scenario. Use [sync::Atomic] instead.
///
/// When pointed-to type is zero-sized, no write is performed.
///
/// ## Example
/// ```no_run
/// use std::mem::{write_volatile, read_volatile};
///
/// static FLAG: i32;
///
/// fn signal_handler() {
///     FLAG.write_volatile(1);
/// }
///
/// while FLAG.read_volatile() == 0 {
///     // wait for something to set the flag
/// }
/// ```
#[inline(ir)]
fn write_volatile<T>(ptr: &mut T, val: T) {
    when !(*ptr is builtins::ZeroSized) {
        *intrinsics::volatile(ptr) = val;
    }
}

/// Performs an unaligned read from a memory location.
///
/// When pointed-to type is zero-sized, no read is performed.
///
/// ## Example
/// ```
/// use std::mem::read_unaligned;
///
/// let a = [255u8, 1, 0, 0, 0];
/// let b: u32 = (&a[1] as &u32).read_unaligned();
///
/// assert_eq!(b, 1);
/// ```
#[inline]
fn read_unaligned<T>(ptr: &T) -> T {
    when *ptr is builtins::ZeroSized {
        util::unit()
    } else {
        let ret: T;
        libc::memcpy(&ret as &mut void, ptr as &void, size_of::<T>());
        ret
    }
}

/// Performs an unaligned write to a memory location.
///
/// When pointed-to type is zero-sized, no write is performed.
///
/// ## Example
/// ```
/// use std::mem::write_unaligned;
///
/// let a = [0u8, 0, 0, 0, 0];
/// (&a[1] as &mut u32).write_unaligned(2);
///
///
/// assert_eq!(a, [0, 2, 0, 0, 0]);
/// ```
#[inline]
fn write_unaligned<T>(ptr: &mut T, val: T) {
    when !(*ptr is builtins::ZeroSized) {
        libc::memcpy(ptr as &mut void, &val as &void, size_of::<T>());
    }
}


/// Returns a dangling non-null pointer.
///
/// The pointer is appropriately aligned for the provided type, and is non-null.
///
/// This can be used as a sentinel value for e.g. collections of zero-sized types or
/// empty slices where the pointer is never dereferenced, but `null` cannot be used
/// for whatever reason.
///
/// This is the pointer that is returned when taking an address of a value that has a
/// zero-sized type.
///
/// If the pointer points to a sized type, dereferencing it is undefined behavior. If
/// the pointer points to a zero-sized type, dereferencing it is no-op.
///
/// ## Example
/// ```
/// use std::mem::dangling;
///
/// struct Foo { a: (), b: [u64; 0] }
///
/// let foo: Foo;
///
/// assert_eq!(&foo.a, dangling::<&mut ()>());
/// assert_eq!(&foo.b, dangling::<&mut [u64; 0]>());
///
/// // Because the types have different alignment. Currently `dangling`
/// // just returns the alignment of the type (e.g. 1, 2, 4, ...)
/// // cast to the pointer type.
/// assert_ne!(&foo.a, &foo.b as &mut void);
/// ```
#[inline(ir)]
fn dangling<Ptr: builtins::Pointer>() -> Ptr {
    intrinsics::dangling::<Ptr>()
}

#[cfg(all(test, test_std))]
#[docs(hide)]
mod tests {
    #[test]
    fn slice_range_index() {
        let a = [1, 2, 3, 4, 5].as_slice();

        assert_eq!(a[..], &[1, 2, 3, 4, 5]);
        assert_eq!(a[..3], &[1, 2, 3]);
        assert_eq!(a[1..], &[2, 3, 4, 5]);
        assert_eq!(a[1..5], &[2, 3, 4, 5]);

        assert_eq!(a[..=3], &[1, 2, 3, 4]);
        assert_eq!(a[1..=3], &[2, 3, 4]);
    }

    #[test]
    fn size_of_zst() {
        assert_eq!(size_of::<()>(), 0usize);
        assert_eq!(size_of::<[u8; 0]>(), 0usize);
        assert_eq!(size_of::<((), (), ())>(), 0usize);
    }

    #[test]
    fn align_of_zst() {
        assert_eq!(align_of::<()>(), 1usize);
        assert_eq!(align_of::<[u8; 0]>(), 1usize);
        assert_eq!(align_of::<((), (), ())>(), 1usize);
    }

    #[test]
    fn test_fill() {
        let a: [u8; 10];
        a[..].fill('a');

        assert_eq!(a[..] as &[u8], "aaaaaaaaaa");
    }

    #[test]
    fn test_fill_custom() {
        let a: [(i32, i32); 5];
        a[..].fill((1, 2));

        assert_eq!(a[..], &[(1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]);
    }


    #[test]
    fn test_copy_nonoverlapping() {
        let a: [u8; 32];
        "hello world".copy_to_nonoverlapping(&a[0]);

        assert_eq!(a[..11] as &[u8], "hello world");
    }

    #[test]
    fn test_copy() {
        let a: [u8; 32];

        "hello world".copy_to_nonoverlapping(&a[0]);
        a[0..11].copy_to(&a[6]);

        assert_eq!(a[0..17] as &[u8], "hello hello world");
    }

    #[test]
    fn test_slice_equals() {
        assert_eq!("hello", "hello");
        assert_ne!("hello", "hella");
        assert_ne!("hello", "hell");
        assert_ne!("hell", "hello");

        assert_eq!([1,2,3,4].as_slice(), [1,2,3,4].as_slice());
        assert_ne!([1,2,3,4].as_slice(), [1,2,3,5].as_slice());
        assert_ne!([1,2,3,4].as_slice(), [1,2,3].as_slice());
        assert_ne!([1,2,3].as_slice(), [1,2,3,4].as_slice());
    }

    #[test]
    fn test_slice_compare() {
        assert!("hello" as &[u8] > "hella" as &[u8]);
        assert!("hello" as &[u8] > "hell" as &[u8]);
        assert!("hell" as &[u8] < "hello" as &[u8]);

        assert!([1,2,3,4].as_slice() > [1,2,3,2].as_slice());
        assert!([1,2,3,4].as_slice() > [1,2,3].as_slice());
        assert!([1,2,3].as_slice() < [1,2,3,4].as_slice());
    }

    #[test]
    #[allow(pure_statement)]
    fn test_bounds_check() {
        let a = [1,2,3,4,5].as_slice();
        a[4];
        a[0..5];
        a[..5];
        a[5..];
        a[..=4];
        a[0..=4];
        a[0..=0];
        a[..=0];
    }

    #[cfg(debug)]

    {   #[test]
        fn test_bounds_check_1() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[5]);
        }

        #[test]
        fn test_bounds_check_2() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[4..3]);
        }

        #[test]
        fn test_bounds_check_3() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[6..]);
        }

        #[test]
        fn test_bounds_check_4() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[..6]);
        }

        #[test]
        fn test_bounds_check_5() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[..=5]);
        }

        #[test]
        fn test_bounds_check_6() {
            let a = [1,2,3,4,5].as_slice();
            test::assert_panics!(a[0..=5]);
        }
    }

    #[test]
    fn test_stack_alloc() {
        let a: &mut [i32] = stack_alloc(10);

        (0..10).fill_slice(a);

        assert_eq!(a, &[0,1,2,3,4,5,6,7,8,9]);
    }

    #[test]
    #[allow(pure_statement)]
    fn test_const_alloc() {
        runtime::const_eval!({
            let a = alloc::<i32>();
            *a = 42;
            assert_eq!(*a, 42);
            free(a);
        });
    }

    #[test]
    #[allow(pure_statement)]
    fn test_const_alloc_zeroed() {
        runtime::const_eval!({
            let a = alloc_zeroed::<i32>();
            assert_eq!(*a, 0);
            free(a);
        });
    }

    #[test]
    #[allow(pure_statement)]
    fn test_const_stack_alloc() {
        runtime::const_eval!({
            let a = stack_alloc::<i32>(10);
            for (idx, ptr) in a.iter_mut().enumerate() {
                *ptr = idx as i32;
            }

            assert_eq!(a, &[0,1,2,3,4,5,6,7,8,9]);
        });
    }

    #[test]
    #[allow(pure_statement)]
    fn test_const_slice_alloc() {
        runtime::const_eval!({
            let a = slice::alloc::<i32>(10);
            for (idx, ptr) in a.iter_mut().enumerate() {
                *ptr = idx as i32;
            }

            assert_eq!(a, &[0,1,2,3,4,5,6,7,8,9]);
            a.free();
        });
    }

    #[test]
    #[allow(pure_statement)]
    fn test_const_slice_alloc_zeroed() {
        runtime::const_eval!({
            let a = slice::alloc_zeroed::<i32>(10);
            assert_eq!(a, &[0,0,0,0,0,0,0,0,0,0]);
            a.free();
        });
    }

    #[test]
    fn test_zeroed() {
        struct Struct {
            a: u32,
            b: u32,
        }

        union Union {
            a: u32,
            b: u32,
        }

        assert_eq!(zeroed::<bool>(), false);
        assert_eq!(zeroed::<u8>(), 0);
        assert_eq!(zeroed::<u16>(), 0);
        assert_eq!(zeroed::<u32>(), 0);
        assert_eq!(zeroed::<u64>(), 0);
        assert_eq!(zeroed::<usize>(), 0);
        assert_eq!(zeroed::<i8>(), 0);
        assert_eq!(zeroed::<i16>(), 0);
        assert_eq!(zeroed::<i32>(), 0);
        assert_eq!(zeroed::<i64>(), 0);
        assert_eq!(zeroed::<isize>(), 0);
        assert_eq!(zeroed::<f32>(), 0.0);
        assert_eq!(zeroed::<f64>(), 0.0);

        assert_eq!(zeroed::<&u8>(), null);
        assert_eq!(zeroed::<&mut u8>(), null);

        let construct = zeroed::<Struct>();
        assert_eq!(construct.a, 0);
        assert_eq!(construct.b, 0);

        let funion = zeroed::<Union>();
        assert_eq!(funion.a, 0);
        assert_eq!(funion.b, 0);

        assert_eq!(zeroed::<[i32; 4]>(), [0,0,0,0]);
        assert_eq!(zeroed::<[i32; 0]>(), []);

        assert_eq!(zeroed::<(i32, u8, bool)>(), (0, 0, false));
    }

    #[test]
    fn test_zeroed_closure() {
        let a = 42;

        let closure = |=a| -> i32 { a };
        let zeroed = zeroed::<typeof(closure)>();

        assert_eq!(closure(), 42);
        assert_eq!(zeroed(), 0);
    }

    #[test]
    fn test_volatile() {
        let a = 42;

        assert_eq!(a.read_volatile(), 42);
        a.write_volatile(0);

        assert_eq!(a, 0);
    }

    #[test]
    fn test_volatile_zst() {
        let a = ();

        assert_eq!(a.read_volatile(), ());
        a.write_volatile(());

        assert_eq!(a, ());
    }
}
