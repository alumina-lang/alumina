//! Thread synchronization primitives

use std::builtins::{Primitive, ZeroSized, Integer};

/// Memory ordering
///
/// Follows C++ memory model ([see here](https://en.cppreference.com/w/c/atomic/memory_order)).
enum Ordering {
    Relaxed = 0,
    // Consume = 1, but it's not included
    Acquire = 2,
    Release = 3,
    AcqRel = 4,
    SeqCst = 5
}

/// Values that can be operated on atomically.
///
/// `Atomic` supports all primitive types (integer types, pointers, [bool]), with the exception of
/// zero-sized types (e.g. [void]). Some types (e.g. [u128] and [i128]) may not be available on all
/// platforms or require library support (`libatomic`).
///
/// ## Example
/// ```
/// use std::sync::{Atomic, Ordering};
///
/// let a: Atomic<i32> = Atomic::new(5);
/// assert_eq!(a.exchange(10, Ordering::Relaxed), 5);
/// assert_eq!(a.load(Ordering::Relaxed), 10);
/// ```
#[transparent]
struct Atomic<T: Primitive + !ZeroSized> {
    _inner: T
}

impl Atomic<T: Primitive + !ZeroSized> {
    /// Create a new `Atomic` value with the given initial value.
    #[inline(ir)]
    fn new(inner: T) -> Atomic<T> {
        Atomic { _inner: inner }
    }

    /// Convert a pointer to a value into a `Atomic` pointer referencing the same memory.
    ///
    /// The pointed-to value needs to be sufficiently aligned for the target platform.
    ///
    /// ## Example
    /// ```
    /// use std::sync::{Atomic, Ordering};
    ///
    /// let i = 42;
    /// let a = Atomic::from_mut_ptr(&i);
    ///
    /// assert_eq!(a.exchange(10, Ordering::Relaxed), 42);
    /// assert_eq!(i, 10);
    /// ```
    #[inline]
    fn from_mut_ptr(inner: &mut T) -> &mut Atomic<T> {
        util::cast(inner)
    }

    /// Convert a slice of values into a slice of `Atomic` values referencing the same memory.
    ///
    /// The pointed-to values need to be sufficiently aligned for the target platform.
    ///
    /// ## Example
    /// ```
    /// use std::sync::{Atomic, Ordering};
    ///
    /// let arr = [1, 2, 3];
    /// let a = Atomic::from_mut_slice(&arr);
    ///
    /// for i in a.iter_mut() {
    ///     i.fetch_add(1, Ordering::Relaxed);
    /// }
    ///
    /// assert_eq!(arr[..], &[2, 3, 4]);
    /// ```
    #[inline]
    fn from_mut_slice(inner: &mut [T]) -> &mut [Atomic<T>] {
        mem::slice::from_raw(&inner[0] as &mut Atomic<T>, inner.len())
    }

    /// Loads a value from the atomic variable.
    #[inline(ir)]
    fn load(self: &Atomic<T>, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            self._inner
        } else {
            intrinsics::codegen_func::<T>("__atomic_load_n", &self._inner, ordering as libc::c_int)
        }
    }

    /// Stores a value into the atomic variable.
    #[inline(ir)]
    fn store(self: &mut Atomic<T>, value: T, ordering: Ordering) {
        if runtime::in_const_context() {
            self._inner = value;
        } else {
            intrinsics::codegen_func::<void>("__atomic_store_n", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Stores a value into the atomic variable, atomically returning the old value.
    #[inline(ir)]
    fn exchange(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            mem::replace(&self._inner, value)
        } else {
            intrinsics::codegen_func::<T>("__atomic_exchange_n", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically compares the value in the atomic variable to `expected` and, if they are equal,
    /// sets the atomic variable to `desired`.
    ///
    /// This is the strong version, which will not fail spuriously. See also [compare_exchange_weak].
    ///
    /// Returns previous value as an `Result::ok(...)` variant if the value was updated and as
    /// an `Result::err(...)` if it was not.
    ///
    /// ## Example
    /// ```
    /// use std::sync::{Atomic, Ordering};
    ///
    /// /// Atomic version of a square root using a CAS loop
    /// fn fetch_sqrt(val: &mut Atomic<u64>, ordering: Ordering) -> u64 {
    ///     let old = val.load(Ordering::Relaxed);
    ///     loop {
    ///         let root = (old as f64).sqrt() as u64;
    ///
    ///         let ret = val.compare_exchange(old, root, ordering, Ordering::Relaxed);
    ///         if ret.is_ok() {
    ///             return old;
    ///         } else {
    ///             old = ret.unwrap_err();
    ///         }
    ///     }
    /// }
    ///
    /// let val = Atomic::new(144u64);
    ///
    /// assert_eq!(val.fetch_sqrt(Ordering::Relaxed), 144u64);
    /// assert_eq!(val.load(Ordering::Relaxed), 12u64);
    /// ```
    #[inline(always)]
    fn compare_exchange(
        self: &mut Atomic<T>,
        expected: T,
        desired: T,
        success_ordering: Ordering,
        failure_ordering: Ordering
    ) -> Result<T, T> {
        if runtime::in_const_context() {
            if self._inner == expected {
                self._inner = desired;
                Result::ok(expected)
            } else {
                Result::err(self._inner)
            }
        } else {
            if intrinsics::codegen_func::<bool>(
                "__atomic_compare_exchange_n",
                &self._inner,
                &expected,
                desired,
                false,
                success_ordering as libc::c_int,
                failure_ordering as libc::c_int
            ) {
                Result::ok(expected)
            } else {
                Result::err(expected)
            }
        }
    }

    /// Atomically compares the value in the atomic variable to `expected` and, if they are equal,
    /// sets the atomic variable to `desired`.
    ///
    /// This is the weak version, which can fail spuriously even when the comparison succeeds.
    ///
    /// Returns previous value as an `Result::ok(...)` variant if the value was updated and as
    /// an `Result::err(...)` if it was not.
    #[inline(always)]
    fn compare_exchange_weak(
        self: &mut Atomic<T>,
        expected: T,
        desired: T,
        success_ordering: Ordering,
        failure_ordering: Ordering
    ) -> Result<T, T> {
        if runtime::in_const_context() {
            self.compare_exchange(expected, desired, success_ordering, failure_ordering)
        } else {
            if intrinsics::codegen_func::<bool>(
                "__atomic_compare_exchange_n",
                &self._inner,
                &expected,
                desired,
                true,
                success_ordering as libc::c_int,
                failure_ordering as libc::c_int
            ) {
                Result::ok(expected)
            } else {
                Result::err(expected)
            }
        }
    }

    /// Updates the value of the atomic variable using a callback function.
    ///
    /// The callback function is passed the current value of the variable and returns the desired value
    /// or `Option::none()` to leave the value unchanged.
    ///
    /// Returns previous value as an `Result::ok(...)` variant if the value was updated and as
    /// an `Result::err(...)` if it was not.
    ///
    /// ## Example
    /// ```
    /// use std::sync::{Atomic, Ordering::SeqCst};
    ///
    /// // Simulate a Collatz sequence starting with 39
    /// let val = Atomic::new(39);
    /// loop {
    ///     let value = val.fetch_update(SeqCst, SeqCst, |v: i32| -> Option<i32> {
    ///         if v == 1 {
    ///             Option::none()
    ///         } else if v % 2 == 0 {
    ///             Option::some(v / 2)
    ///         } else {
    ///             Option::some(v * 3 + 1)
    ///         }
    ///     });
    ///
    ///     if value.is_ok() {
    ///         println!("val = {}", value.unwrap());
    ///     } else {
    ///         println!("val = {}. We are done.", value.unwrap_err());
    ///         break;
    ///     }
    /// }
    /// ```
    #[inline]
    fn fetch_update<F: Fn(T) -> Option<T>>(
        self: &mut Atomic<T>,
        set_order: Ordering,
        fetch_order: Ordering,
        f: F
    ) -> Result<T, T> {
        let prev = self.load(fetch_order);
        loop {
            let updated = f(prev);
            if updated.is_none() {
                return Result::err(prev);
            }
            let maybe_updated = self.compare_exchange_weak(
                prev,
                updated.unwrap(),
                set_order,
                fetch_order,
            );
            if maybe_updated.is_ok() {
                return Result::ok(prev);
            } else {
                prev = maybe_updated.unwrap_err();
            }
        }
    }

}

impl Atomic<T: Integer> {
    /// Atomically adds `value` to the atomic variable and returns the old value.
    #[inline(ir)]
    fn fetch_add(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { x + y })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_add", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically subtracts `value` from the atomic variable and returns the old value.
    #[inline(ir)]
    fn fetch_sub(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { x - y })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_sub", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically performs `*self &= value` and returns the old value.
    #[inline(ir)]
    fn fetch_and(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { x & y })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_and", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically performs `*self |= value` and returns the old value.
    #[inline(ir)]
    fn fetch_or(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { x | y })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_or", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically performs `*self ^= value` and returns the old value.
    #[inline(ir)]
    fn fetch_xor(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { x ^ y })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_xor", &self._inner, value, ordering as libc::c_int)
        }
    }

    /// Atomically performs `*self = ~(*self & value)` and returns the old value.
    #[inline(ir)]
    fn fetch_nand(self: &mut Atomic<T>, value: T, ordering: Ordering) -> T {
        if runtime::in_const_context() {
            internal::const_atomic_update(self, value, |x: T, y: T| -> T { ~(x & y) })
        } else {
            intrinsics::codegen_func::<T>("__atomic_fetch_nand", &self._inner, value, ordering as libc::c_int)
        }
    }
}

/// Memory barrier
#[inline(ir)]
fn fence(ordering: Ordering) {
    if !runtime::in_const_context() {
        intrinsics::codegen_func::<void>("__atomic_thread_fence", ordering as libc::c_int)
    }
}

/// Compiler-only memory barrier
///
/// This prevents the compiler from reordering memory accesses according to the
/// memory access ordering, but does not prevent hardware reordering.
#[inline(ir)]
fn compiler_fence(ordering: Ordering) {
    if !runtime::in_const_context() {
        intrinsics::codegen_func::<void>("__atomic_thread_fence", ordering as libc::c_int)
    }
}

/// A hint to the CPU that we are spinning
///
/// This does not result in a system call like [thread::yield_thread], but can reduce power usage
/// or allow other hyperthreads to run.
///
/// ## Example
/// ```no_run
/// use std::sync::{Atomic, Ordering, spin_loop};
///
/// let flag = Atomic::new(false);
/// while !flag.exchange(true, Ordering::Acquire) {
///     spin_loop();
/// }
/// ```
#[inline(ir)]
fn spin_loop() {
    if !runtime::in_const_context() {
        #[cfg(target_arch = "x86")]
        std::intrinsics::asm("pause");
        #[cfg(target_arch = "x86_64")]
        std::intrinsics::asm("pause");
        #[cfg(target_arch = "arm")]
        std::intrinsics::asm("yield");
        #[cfg(target_arch = "aarch64")]
        std::intrinsics::asm("isb sy");
    }
}

/// Protocol for types implementing mutex-like semantics.
///
/// Implemented by e.g. [Mutex] and [Spinlock].
protocol Lockable<Self> {
    /// Acquires the lock
    fn lock(self: &mut Self);
    /// Releases the lock
    fn unlock(self: &mut Self);
}

/// Execute a closure with the lock held
///
/// ## Example
/// ```
/// use std::sync::{Mutex, with_lock};
///
/// let mutex = Mutex::new();
/// mutex.with_lock(|| {
///     println!("Hello, World");
/// });
/// ```
fn with_lock<L: Lockable<L>, T, F: Fn() -> T>(lock: &mut L, f: F) -> T {
    lock.lock();
    defer lock.unlock();

    f()
}

/// A mutual exclusion lock (mutex)
///
/// This is a standard pthread mutex.
///
/// ## Example
/// ```
/// use std::sync::Mutex;
/// use std::thread::spawn;
///
/// let mutex = Mutex::new();
/// let sync_print = |&mutex, t: &[u8]| {
///     mutex.lock();
///     defer mutex.unlock();
///
///     println!("{}", t);
/// };
///
/// let t1 = spawn(|=sync_print| { sync_print("hello world 1") });
/// let t2 = spawn(|=sync_print| { sync_print("hello world 2") });
///
/// t1.join().unwrap();
/// t2.join().unwrap();
/// ```
struct Mutex {
    inner: libc::pthread_mutex_t
}

impl Mutex {
    use std::io::Error;

    /// Creates a new mutex.
    fn new() -> Mutex {
        thread::internal::ensure_threading!();

        Mutex {
            inner: libc::PTHREAD_MUTEX_INITIALIZER
        }
    }

    /// Acquires a mutex, blocking the current thread until it can acquire it.
    fn lock(self: &mut Mutex) {
        let ret = libc::pthread_mutex_lock(&self.inner);
        if ret != 0 {
            panic!("pthread_mutex_lock failed: {}", Error::from_errno_custom(ret));
        }
    }

    /// Attempts to acquire a mutex, but does not block.
    ///
    /// Returns `true` if the mutex was successfully acquired, `false` otherwise.
    fn try_lock(self: &mut Mutex) -> bool {
        let ret = libc::pthread_mutex_trylock(&self.inner);
        switch ret {
            0 => true,
            libc::EBUSY => false,
            _ => panic!("pthread_mutex_trylock failed: {}", Error::from_errno_custom(ret))
        }
    }

    /// Releases a mutex.
    fn unlock(self: &mut Mutex) {
        let ret = libc::pthread_mutex_unlock(&self.inner);
        if ret != 0 {
            panic!("pthread_mutex_unlock failed: {}", Error::from_errno_custom(ret));
        }
    }
}

/// Reader-writer lock
///
/// This is a standard pthread rwlock.
struct RwLock {
    inner: libc::pthread_rwlock_t
}

impl RwLock {
    use std::io::Error;

    /// Creates a new reader-writer lock.
    fn new() -> RwLock {
        thread::internal::ensure_threading!();

        RwLock {
            inner: libc::libc::PTHREAD_RWLOCK_INITIALIZER
        }
    }

    /// Acquires a reader-writer lock for reading.
    fn read_lock(self: &mut RwLock) {
        let ret = libc::pthread_rwlock_rdlock(&self.inner);
        if ret != 0 {
            panic!("pthread_rwlock_rdlock failed: {}", Error::from_errno_custom(ret));
        }
    }

    /// Attempts to acquire a reader-writer lock for reading.
    ///
    /// Returns `true` if the lock was successfully acquired, `false` otherwise.
    fn try_read_lock(self: &mut RwLock) -> bool {
        let ret = libc::pthread_rwlock_tryrdlock(&self.inner);
        switch ret {
            0 => true,
            libc::EBUSY => false,
            _ => panic!("pthread_rwlock_tryrdlock failed: {}", Error::from_errno_custom(ret))
        }
    }

    /// Acquires a reader-writer lock for writing.
    fn write_lock(self: &mut RwLock) {
        let ret = libc::pthread_rwlock_wrlock(&self.inner);
        if ret != 0 {
            panic!("pthread_rwlock_wrlock failed: {}", Error::from_errno_custom(ret));
        }
    }

    /// Attempts to acquire a reader-writer lock for writing.
    ///
    /// Returns `true` if the lock was successfully acquired, `false` otherwise.
    fn try_write_lock(self: &mut RwLock) -> bool {
        let ret = libc::pthread_rwlock_trywrlock(&self.inner);
        switch ret {
            0 => true,
            libc::EBUSY => false,
            _ => panic!("pthread_rwlock_trywrlock failed: {}", Error::from_errno_custom(ret))
        }
    }

    /// Releases a reader-writer lock.
    fn unlock(self: &mut RwLock) {
        let ret = libc::pthread_rwlock_unlock(&self.inner);
        if ret != 0 {
            panic!("pthread_rwlock_unlock failed: {}", Error::from_errno_custom(ret));
        }
    }
}

/// Condition variable
///
/// This is a standard pthread condition variable.
struct CondVar {
    inner: libc::pthread_cond_t
}

impl CondVar {
    use std::io::Error;

    /// Creates a new condition variable.
    fn new() -> CondVar {
        thread::internal::ensure_threading!();

        CondVar {
            inner: libc::PTHREAD_COND_INITIALIZER
        }
    }

    /// Waits on a condition variable.
    ///
    /// May return spuriously.
    fn wait(self: &mut CondVar, mutex: &mut Mutex) {
        let ret = libc::pthread_cond_wait(&self.inner, &mutex.inner);
        if ret != 0 {
            panic!("pthread_cond_wait failed: {}", Error::from_errno_custom(ret));
        }
    }

    /// Waits on a condition variable with a timeout.
    ///
    /// Returns `true` if the condition variable was signaled, `false` otherwise. However, since the
    /// condition variable can wake up spuriously, it is not guaranteed that the condition is actually
    /// satisfied.
    fn wait_timeout(self: &mut CondVar, mutex: &mut Mutex, timeout: time::Duration) -> bool {
        if timeout.is_negative() {
            return false;
        }

        // We use the wall time here, not the monotonic time that's used for time::Instant
        let cur_time: libc::timeval;
        assert!(libc::gettimeofday(&cur_time, null) == 0);

        let timespec = libc::timespec {
            tv_sec: util::cast(cur_time.tv_sec) + util::cast(timeout.secs),
            tv_nsec: util::cast(cur_time.tv_usec) * 1000 + util::cast(timeout.nanos)
        };

        if timespec.tv_nsec >= util::cast(time::NANOS_PER_SEC) {
            timespec.tv_sec += 1;
            timespec.tv_nsec -= util::cast(time::NANOS_PER_SEC);
        }

        let ret = libc::pthread_cond_timedwait(&self.inner, &mutex.inner, &timespec);
        switch ret {
            0 => true,
            libc::ETIMEDOUT => false,
            _ => panic!("pthread_cond_timedwait failed: {}", Error::from_errno_custom(ret))
        }
    }

    /// Signals a condition variable, waking up one waiting thread.
    fn notify_one(self: &mut CondVar) {
        let ret = libc::pthread_cond_signal(&self.inner);
        if ret != 0 {
            panic!("pthread_cond_signal failed: {}", Error::from_errno_custom(ret));
        }
    }

    /// Signals a condition variable, waking up all waiting threads.
    fn notify_all(self: &mut CondVar) {
        let ret = libc::pthread_cond_broadcast(&self.inner);
        if ret != 0 {
            panic!("pthread_cond_broadcast failed: {}", Error::from_errno_custom(ret));
        }
    }
}

/// A spinlock.
///
/// Dangerous to use in userspace, do not use unless you know what you're doing. Can lead to priority
/// inversion and other nasty stuff. Prefer to use [Mutex] instead.
///
/// ## Example
/// ```dubious
/// use std::sync::Spinlock;
/// use std::thread::spawn;
///
/// let lock = Spinlock::new();  // prefer to use Mutex instead
/// let sync_print = |&lock, t: &[u8]| {
///     lock.lock();
///     defer lock.unlock();
///
///     println!("{}", t);
/// };
///
/// let t1 = spawn(|=sync_print| { sync_print("hello world 1") });
/// let t2 = spawn(|=sync_print| { sync_print("hello world 2") });
///
/// t1.join().unwrap();
/// t2.join().unwrap();
/// ```
struct Spinlock {
    inner: Atomic<bool>
}

impl Spinlock {
    /// Creates a new mutex.
    fn new() -> Spinlock {
        Spinlock {
            inner: Atomic::new(false)
        }
    }

    /// Acquires a spinlock, spinning until it is acquired.
    #[inline]
    fn lock(self: &mut Spinlock) {
        while self.inner.exchange(true, Ordering::Acquire) {
            while self.inner.load(Ordering::Relaxed) {
                spin_loop();
            }
        }
    }

    /// Attempts to acquire a spinlock, but does not spin.
    ///
    /// Returns `true` if the lock was successfully acquired, `false` otherwise.
    #[inline]
    fn try_lock(self: &mut Spinlock) -> bool {
        !self.inner.load(Ordering::Relaxed)
            && !self.inner.exchange(true, Ordering::Acquire)
    }

    /// Attempts to acquire a spinlock, spining for up to `times` times.
    ///
    /// Returns `true` if the lock was successfully acquired, `false` otherwise.
    #[inline]
    fn try_lock_bounded(self: &mut Spinlock, times: usize) -> bool {
        while times > 0 {
            times -= 1;
            if !self.inner.exchange(true, Ordering::Acquire) {
                return true;
            }
            while times > 0 && self.inner.load(Ordering::Relaxed) {
                times -= 1;
                spin_loop();
            }
        }
        false
    }

    /// Releases a spinlock.
    #[inline]
    fn unlock(self: &mut Spinlock) {
        self.inner.store(false, Ordering::Release);
    }
}


/// A synchronization primitive that allows multiple threads to wait until the
/// event is signalled.
///
/// Once an event is signalled it can be reset and reused.
///
/// ## Example
/// ```
/// use std::sync::Event;
/// use std::thread::spawn;
///
/// let ev = Event::new();
/// let text: &[u8];
///
/// let t = spawn(|&ev, &text| {
///     text = "Hello, world!";
///     ev.set();
/// });
///
/// ev.wait();
/// assert_eq!(text, "Hello, world!");
///
/// t.join().unwrap();
/// ```
struct Event {
    state: Atomic<usize>
}

impl Event {
    use internal::{EventWaiter, EVENT_SET, EVENT_RESET};

    /// Create a new event in an unset state
    fn new() -> Event {
        Event {
            state: Atomic::new(EVENT_RESET)
        }
    }

    /// Create a new event in a set state
    fn new_set() -> Event {
        Event {
            state: Atomic::new(EVENT_SET)
        }
    }

    /// Signals the event, waking up all threads that are waiting on it.
    fn set(self: &mut Event) {
        let state = self.state.exchange(EVENT_SET, Ordering::AcqRel);
        switch state {
            EVENT_SET, EVENT_RESET => {}
            _ => {
                // Wake up some threads
                let waiter = state as &mut EventWaiter;
                while waiter != null {
                    /// As soon as we set the `signaled` flag, the region of
                    /// memory that waiter points to may be invalidated. So we need
                    /// to store it first.
                    let next = waiter.next;
                    let thread = waiter.thread;
                    waiter.signaled.store(true, Ordering::Release);
                    thread.unpark();
                    waiter = next;
                }
            }
        }
    }

    /// Resets the event, so that it can be waited on and signalled again.
    ///
    /// If event is not in set state, this is a no-op.
    fn reset(self: &mut Event) {
        let _ = self.state.compare_exchange(
            EVENT_SET,
            EVENT_RESET,
            Ordering::Release,
            Ordering::Relaxed
        );
    }

    /// Checks if the event is signalled.
    fn is_set(self: &Event) -> bool {
        self.state.load(Ordering::Acquire) == EVENT_SET
    }

    /// Waits for the event to be signalled.
    fn wait(self: &mut Event) {
        let state = self.state.load(Ordering::Acquire);
        loop {
            if state == EVENT_SET {
                return;
            }

            // This is a clever idea I stole from Rust's Once, a linked-list of nodes
            // on the stack. Since the threads are sleeping, the address will
            // remain valid.
            let waiter = EventWaiter {
                thread: thread::Thread::current(),
                signaled: Atomic::new(false),
                next: state as &mut EventWaiter
            };

            let maybe_replaced = self.state.compare_exchange_weak(
                state,
                &waiter as usize,
                Ordering::Release,
                Ordering::Relaxed
            );

            if maybe_replaced.is_err() {
                // Someone beat us to the punch
                state = maybe_replaced.unwrap_err();
                continue;
            }

            while !waiter.signaled.load(Ordering::Acquire) {
                thread::Thread::park();
            }

            return;
        }
    }
}

/// A one-shot channel (future).
///
/// `Oneshot` can be used to send a single value between threads. Multiple threads can wait on it and
/// they will all receive the same value.
///
/// ## Example
/// ```
/// use std::sync::Oneshot;
/// use std::thread::spawn;
///
/// let one_shot: Oneshot<i32> = Oneshot::new();
/// let t = spawn(|&one_shot| {
///     one_shot.send(42);
/// });
///
/// let value = one_shot.recv();
/// assert_eq!(value, 42);
///
/// t.join().unwrap();
/// ```
struct Oneshot<T> {
    _event: Event,
    _value: T
}

impl Oneshot<T> {
    /// Creates a new one-shot channel.
    fn new() -> Oneshot<T> {
        Oneshot::<T> {
            _event: Event::new(),
            _value: mem::uninitialized()
        }
    }

    /// Creates a completed one-shot channel from a value
    fn from_value(value: T) -> Oneshot<T> {
        Oneshot::<T> {
            _event: Event::new_set(),
            _value: value
        }
    }

    /// Sets the value of the channel.
    ///
    /// If the channel is already closed, this will return `Result::err(ChannelError::Closed)`.
    ///
    /// It is undefined behavior to call this function more than once on a single channel.
    fn send(self: &mut Oneshot<T>, value: T) {
        debug_assert!(!self._event.is_set());

        self._value = value;
        self._event.set();
    }

    /// Gets the value of the channel.
    ///
    /// If the channel's value has already been set, this will return immediately. Otherwise,
    /// it will block until the value is set.
    fn recv(self: &mut Oneshot<T>) -> T {
        self._event.wait();
        self._value
    }

    /// Tries to get the value of the channel
    ///
    /// If the channel's value has not been set, this will return [channel::ChannelError::WouldBlock].
    fn try_recv(self: &mut Oneshot<T>) -> Result<T, channel::ChannelError> {
        if self._event.is_set() {
            Result::ok(self._value)
        } else {
            Result::err(channel::ChannelError::WouldBlock)
        }
    }
}

#[docs(no_index)]
mod internal {
    const EVENT_RESET: usize = 0x0;
    const EVENT_SET: usize = 0x1;

    #[align(4)]
    struct EventWaiter {
        thread: thread::Thread,
        signaled: Atomic<bool>,
        next: &mut EventWaiter,
    }

    // Used only in the const context, extracted as a function so the main atomic operation
    // can be IR-inlined
    fn const_atomic_update<T>(a: &mut Atomic<T>, b: T, op: fn(T, T) -> T) -> T {
        let old = a._inner;
        a._inner = op(old, b);
        old
    }
}

#[cfg(all(test, test_std))]
#[docs(hide)]
mod tests {
    use time::Duration;

    #[test]
    fn test_ordering_values_match() {
        // This test asserts that the ordering constant values we hardcode above match the
        // builtin constants in the C compiler.

        macro match($a, $b) {
            assert_eq!($a as libc::c_int, intrinsics::codegen_const::<libc::c_int>($b));
        }

        match!(Ordering::Relaxed, "__ATOMIC_RELAXED");
        match!(Ordering::Acquire, "__ATOMIC_ACQUIRE");
        match!(Ordering::Release, "__ATOMIC_RELEASE");
        match!(Ordering::AcqRel, "__ATOMIC_ACQ_REL");
        match!(Ordering::SeqCst, "__ATOMIC_SEQ_CST");
    }

    #[test]
    fn test_load() {
        let a: Atomic<i32> = Atomic::new(42);
        assert_eq!(a.load(Ordering::Relaxed), 42);
    }

    #[test]
    fn test_store() {
        let a: Atomic<i32> = Atomic::new(42);
        a.store(43, Ordering::Relaxed);
        assert_eq!(a.load(Ordering::Relaxed), 43);
    }

    #[test]
    fn test_exchange() {
        let a: Atomic<i32> = Atomic::new(42);
        assert_eq!(a.exchange(43, Ordering::Relaxed), 42);
        assert_eq!(a.load(Ordering::Relaxed), 43);
    }

    #[test]
    fn test_compare_exchange() {
        let a: Atomic<i32> = Atomic::new(42);
        assert_eq!(a.compare_exchange(42, 43, Ordering::Relaxed, Ordering::Relaxed), Result::ok(42));
        assert_eq!(a.load(Ordering::Relaxed), 43);
        assert_eq!(a.compare_exchange(44, 45, Ordering::Relaxed, Ordering::Relaxed), Result::err(43));
        assert_eq!(a.load(Ordering::Relaxed), 43);
    }

    #[test]
    fn test_compare_exchange_weak() {
        let a: Atomic<i32> = Atomic::new(42);
        assert_eq!(a.compare_exchange_weak(42, 43, Ordering::Relaxed, Ordering::Relaxed), Result::ok(42));
        assert_eq!(a.load(Ordering::Relaxed), 43);
        assert_eq!(a.compare_exchange_weak(44, 45, Ordering::Relaxed, Ordering::Relaxed), Result::err(43));
        assert_eq!(a.load(Ordering::Relaxed), 43);
    }

    #[test]
    fn test_fetch_add() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_add(1, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 43u32);
    }

    #[test]
    fn test_fetch_sub() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_sub(1, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 41u32);
    }

    #[test]
    fn test_fetch_and() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_and(0xFu32, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 10u32);
    }

    #[test]
    fn test_fetch_or() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_or(0xFu32, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 47u32);
    }

    #[test]
    fn test_fetch_xor() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_xor(0xFu32, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 37u32);
    }

    #[test]
    fn test_fetch_nand() {
        let a: Atomic<u32> = Atomic::new(42u32);
        assert_eq!(a.fetch_nand(0xFu32, Ordering::Relaxed), 42u32);
        assert_eq!(a.load(Ordering::Relaxed), 0xfffffff5u32);
    }

    #[cfg(threading)]
    #[test]
    fn test_mutex() {
        let a = Mutex::new();
        a.lock();
        thread::spawn(|&a| {
            assert_eq!(a.try_lock(), false);
        }).join().unwrap();
        a.unlock();
        assert_eq!(a.try_lock(), true);
        a.unlock();
    }

    #[cfg(threading)]
    #[test]
    fn test_rwlock() {
        let a = RwLock::new();
        a.read_lock();
        thread::spawn(|&a| {
            assert_eq!(a.try_write_lock(), false);
            assert_eq!(a.try_read_lock(), true);
            a.unlock();
        }).join().unwrap();
        a.unlock();

        a.write_lock();
        thread::spawn(|&a| {
            assert_eq!(a.try_read_lock(), false);
            assert_eq!(a.try_write_lock(), false);
        }).join().unwrap();
        a.unlock();
    }

    #[cfg(threading)]
    #[test]
    fn test_condvar() {
        use thread::{spawn, Thread};

        let counter = 0;

        let flag = Atomic::new(false);
        let mutex = Mutex::new();
        let condvar = CondVar::new();

        let t = spawn(|&counter, &condvar, &mutex, &flag| {
            /// Ensure the main thread gets the mutex first;
            while !flag.load(Ordering::SeqCst) {
                Thread::park();
            }

            mutex.lock();
            defer mutex.unlock();

            counter += 1;
            condvar.notify_one();
        });

        mutex.lock();
        defer mutex.unlock();

        assert!(!condvar.wait_timeout(&mutex, time::Duration::zero()));

        flag.store(true, Ordering::SeqCst);
        t.thread().unpark();

        while counter == 0 {
            condvar.wait(&mutex);
        }

        assert_eq!(mutex.try_lock(), false);

        t.join().unwrap();
        assert_eq!(counter, 1);
    }

    #[cfg(threading)]
    #[test]
    fn test_event() {
        let ev = Event::new();
        assert_eq!(ev.is_set(), false);
        ev.set();
        assert_eq!(ev.is_set(), true);
        ev.wait();
        ev.reset();
        assert_eq!(ev.is_set(), false);
    }

    #[cfg(threading)]
    #[test]
    fn test_event_blocking() {
        use thread::spawn;
        let ev1 = Event::new();
        let ev2 = Event::new();

        let t1 = spawn(|&ev1, &ev2| {
            ev1.wait();
            ev1.reset();
            ev2.set();
        });

        ev1.set();
        ev2.wait();

        assert_eq!(ev1.is_set(), false);
        assert_eq!(ev2.is_set(), true);

        t1.join().unwrap();
    }

    #[cfg(threading)]
    #[test]
    fn test_oneshot() {
        let chan: Oneshot<i32> = Oneshot::new();
        assert_eq!(chan.try_recv(), Result::err(channel::ChannelError::WouldBlock));
        chan.send(1337);
        assert_eq!(chan.try_recv(), Result::ok(1337));
        assert_eq!(chan.recv(), 1337);
    }

    #[cfg(threading)]
    #[test]
    fn test_oneshot_completed() {
        let chan: Oneshot<i32> = Oneshot::from_value(1337);
        assert_eq!(chan.recv(), 1337);
    }

    #[cfg(threading)]
    #[test]
    fn test_oneshot_blocking() {
        use thread::{spawn, yield_thread, Thread};

        let chan: Oneshot<i32> = Oneshot::new();
        let main_thread = Thread::current();
        let flag = Atomic::new(2);

        let t1 = spawn(|&chan, &flag, =main_thread| -> i32 {
            if flag.fetch_sub(1, Ordering::SeqCst) == 1 {
                main_thread.unpark();
            }
            chan.recv() + 1
        });

        let t2 = spawn(|&chan, &flag, =main_thread| -> i32 {
            if flag.fetch_sub(1, Ordering::SeqCst) == 1 {
                main_thread.unpark();
            }
            chan.recv() + 2
        });

        while flag.load(Ordering::SeqCst) != 0 {
            Thread::park();
        }
        /// Make sure the threads are really blocked.
        for _ in 0..100 {
            yield_thread();
        }

        chan.send(42);
        assert_eq!(t1.join().unwrap(), 43);
        assert_eq!(t2.join().unwrap(), 44);
    }

    #[test]
    fn test_spinlock() {
        let lock = Spinlock::new();
        lock.lock();
        lock.unlock();
        assert!(lock.try_lock());
        lock.unlock();
        assert!(lock.try_lock_bounded(50));
        lock.unlock();
    }

    #[test]
    fn test_spinlock_try() {
        let lock = Spinlock::new();
        lock.lock();
        assert!(!lock.try_lock());
    }

    #[test]
    fn test_spinlock_bounded() {
        let lock = Spinlock::new();
        lock.lock();
        assert!(!lock.try_lock_bounded(50));
    }
}
